#!/usr/bin/env python3
"""Monitor S3 workset directories and launch Daylily pipelines automatically."""

from __future__ import annotations

import argparse
import concurrent.futures
import contextlib
import csv
import dataclasses
import datetime as dt
import json
import logging
import os
import re
import shutil
import shlex
import subprocess
import sys
import time
from collections import defaultdict
import textwrap
from pathlib import Path, PurePosixPath
from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Set, Tuple

import boto3
from botocore.exceptions import ClientError
import yaml

from daylib import workset_metrics

LOGGER = logging.getLogger("daylily.workset_monitor")

READY_CLUSTER_STATUSES = {"CREATE_COMPLETE", "UPDATE_COMPLETE"}
READY_COMPUTE_FLEET_STATUSES = {"RUNNING", "ENABLED", "STARTED"}

SENTINEL_FILES = {
    "ready": "daylily.ready",
    "lock": "daylily.lock",
    "in_progress": "daylily.in_progress",
    "error": "daylily.error",
    "complete": "daylily.complete",
    "ignore": "daylily.ignore",
}
OPTIONAL_SENTINELS = {
    SENTINEL_FILES["lock"],
    SENTINEL_FILES["in_progress"],
    SENTINEL_FILES["error"],
    SENTINEL_FILES["complete"],
    SENTINEL_FILES["ignore"],
}
SENTINEL_SUFFIX = tuple(
    "daylily." + suffix
    for suffix in ("ready", "lock", "in_progress", "error", "complete", "ignore")
)

DEFAULT_STAGE_SAMPLES_NAME = "stage_samples.tsv"
WORK_YAML_NAME = "daylily_work.yaml"
INFO_YAML_NAME = "daylily_info.yaml"
SAMPLE_DATA_DIRNAME = "sample_data"
PIPELINE_LOCATION_MARKER = ".daylily-monitor-location"
PIPELINE_SESSION_MARKER = ".daylily-monitor-tmux-session"
PIPELINE_START_MARKER = ".daylily-monitor-pipeline-start"
PIPELINE_SUCCESS_SENTINEL = "daylily.successful_run"
PIPELINE_FAILURE_SENTINEL = "daylily.failed_run"
FSX_EXPORT_STATUS_FILENAME = "fsx_export.yaml"
CLUSTER_STATE_DIR = "_clusters"
CLUSTER_IDLE_MARKER = "idle_since"
WORKSET_CLUSTER_MARKER = ".daylily-monitor-cluster"
REPORT_CACHE_FILENAME = "report_metrics.json"

STATE_PRIORITIES = {
    "error": 0,
    "in-progress": 1,
    "locked": 2,
    "ready": 3,
    "complete": 4,
    "ignored": 5,
    "unknown": 6,
}
STATE_COLORS = {
    "error": "\033[31m",
    "in-progress": "\033[33m",
    "locked": "\033[36m",
    "ready": "\033[34m",
    "complete": "\033[32m",
    "ignored": "\033[90m",
    "unknown": "\033[37m",
}


class MonitorError(RuntimeError):
    """Raised when a workset fails validation or processing."""


class CommandFailedError(MonitorError):
    """Raised when an external command fails."""

    def __init__(self, command_label: str, command_display: str) -> None:
        super().__init__(f"Command failed: {command_display}")
        self.command_label = command_label
        self.command_display = command_display


@dataclasses.dataclass
class AWSConfig:
    profile: str
    region: str
    session_duration_seconds: Optional[int] = None

    def session_kwargs(self) -> Dict[str, str]:
        kwargs: Dict[str, str] = {"region_name": self.region}
        if self.profile:
            kwargs["profile_name"] = self.profile
        return kwargs


@dataclasses.dataclass
class MonitorOptions:
    bucket: str
    prefix: str
    poll_interval_seconds: int = 60
    ready_lock_backoff_seconds: int = 30
    continuous: bool = True
    sentinel_index_prefix: Optional[str] = None

    def normalised_prefix(self) -> str:
        prefix = self.prefix.lstrip("/")
        if prefix and not prefix.endswith("/"):
            prefix += "/"
        return prefix


@dataclasses.dataclass
class ClusterOptions:
    template_path: Optional[str] = None
    preferred_availability_zone: Optional[str] = None
    auto_teardown: bool = False
    idle_teardown_minutes: int = 20
    reuse_cluster_name: Optional[str] = None
    contact_email: Optional[str] = None
    repo_tag: Optional[str] = None


@dataclasses.dataclass
class PipelineOptions:
    workdir: str
    stage_command: str
    clone_command: str
    run_prefix: str
    export_command: str
    pipeline_timeout_minutes: Optional[int] = None
    local_stage_root: Optional[str] = None
    reference_bucket: Optional[str] = None
    ssh_identity_file: Optional[str] = None
    ssh_user: str = "ubuntu"
    ssh_extra_args: List[str] = dataclasses.field(default_factory=list)
    login_shell_init: str = "source ~/.bashrc"
    tmux_session_prefix: str = "daylily"
    tmux_keepalive_shell: str = "bash"
    # Local monitor metadata (markers, tmux name) â€” never /fsx
    local_state_root: Optional[str] = "~/.cache/daylily-monitor"
    # FSx clone base + repo dir name for fallback path computation
    clone_dest_root: str = "/fsx/analysis_results/ubuntu"
    repo_dir_name: str = "daylily-omics-analysis"


@dataclasses.dataclass
class StageArtifacts:
    """Details about staged sample files and manifests."""

    fsx_stage_dir: PurePosixPath
    staged_files: List[PurePosixPath]
    samples_manifest: Optional[PurePosixPath]
    units_manifest: Optional[PurePosixPath]


@dataclasses.dataclass
class Workset:
    name: str
    prefix: str
    sentinels: Dict[str, str]
    has_required_files: bool = False

    def sentinel_timestamp(self, sentinel: str) -> Optional[str]:
        return self.sentinels.get(sentinel)


@dataclasses.dataclass
class WorksetReportRow:
    name: str
    state: str
    start_dt: Optional[dt.datetime]
    start_display: Optional[str]
    detail: Optional[str]
    has_required_files: bool
    display_state: Optional[str] = None
    metrics: Optional["WorksetReportMetrics"] = None

    @property
    def state_text(self) -> str:
        return self.display_state or self.state


@dataclasses.dataclass
class WorksetReportMetrics:
    cluster_name: Optional[str] = None
    region_az: Optional[str] = None
    budget_name: Optional[str] = None
    run_seconds: Optional[float] = None
    end_dt: Optional[dt.datetime] = None
    pipeline_command: Optional[str] = None
    clone_command: Optional[str] = None
    samples: Optional[int] = None
    sample_libraries: Optional[int] = None
    fastq_files: Optional[int] = None
    fastq_bytes: Optional[int] = None
    cram_files: Optional[int] = None
    cram_bytes: Optional[int] = None
    vcf_files: Optional[int] = None
    vcf_bytes: Optional[int] = None
    results_bytes: Optional[int] = None
    s3_daily_cost: Optional[float] = None
    cram_region_cost: Optional[float] = None
    cram_internet_cost: Optional[float] = None
    vcf_region_cost: Optional[float] = None
    vcf_internet_cost: Optional[float] = None
    ec2_cost: Optional[float] = None

    def as_dict(self) -> Dict[str, Optional[object]]:
        return {
            "cluster_name": self.cluster_name,
            "region_az": self.region_az,
            "budget_name": self.budget_name,
            "run_seconds": self.run_seconds,
            "end_dt": self.end_dt.isoformat() if self.end_dt else None,
            "pipeline_command": self.pipeline_command,
            "clone_command": self.clone_command,
            "samples": self.samples,
            "sample_libraries": self.sample_libraries,
            "fastq_files": self.fastq_files,
            "fastq_bytes": self.fastq_bytes,
            "cram_files": self.cram_files,
            "cram_bytes": self.cram_bytes,
            "vcf_files": self.vcf_files,
            "vcf_bytes": self.vcf_bytes,
            "results_bytes": self.results_bytes,
            "s3_daily_cost": self.s3_daily_cost,
            "cram_region_cost": self.cram_region_cost,
            "cram_internet_cost": self.cram_internet_cost,
            "vcf_region_cost": self.vcf_region_cost,
            "vcf_internet_cost": self.vcf_internet_cost,
            "ec2_cost": self.ec2_cost,
        }


@dataclasses.dataclass
class ReportColumn:
    title: str
    value_func: Callable[[WorksetReportRow], object]
    formatter: Optional[Callable[[str, WorksetReportRow], str]] = None
    min_width: int = 0


@dataclasses.dataclass
class MonitorConfig:
    aws: AWSConfig
    monitor: MonitorOptions
    cluster: ClusterOptions
    pipeline: PipelineOptions

    @staticmethod
    def load(path: Path) -> "MonitorConfig":
        with path.open("r", encoding="utf-8") as handle:
            data = yaml.safe_load(handle)
        aws_cfg = AWSConfig(**data["aws"])
        monitor_cfg = MonitorOptions(**data["monitor"])
        cluster_cfg = ClusterOptions(**data.get("cluster", {}))
        pipeline_cfg = PipelineOptions(**data["pipeline"])
        return MonitorConfig(
            aws=aws_cfg, monitor=monitor_cfg, cluster=cluster_cfg, pipeline=pipeline_cfg
        )


class WorksetMonitor:
    def __init__(
        self,
        config: MonitorConfig,
        *,
        dry_run: bool = False,
        debug: bool = False,
        process_directories: Optional[Sequence[str]] = None,
        attempt_restart: bool = False,
        wrap_char: int = 100,
    ) -> None:
        self.config = config
        self.dry_run = dry_run
        self.debug = debug
        self.attempt_restart = attempt_restart
        self._wrap_char = wrap_char

        self._session = boto3.session.Session(**config.aws.session_kwargs())
        self._s3 = self._session.client("s3")
        self._sts = self._session.client("sts")
        self._sentinel_history: Dict[str, Dict[str, str]] = {}
        self._process_directories: Optional[Set[str]] = (
            {name.strip() for name in process_directories if name.strip()}
            if process_directories
            else None
        )
        self._headnode_ips: Dict[str, str] = {}
        self._pipeline_locations: Dict[str, PurePosixPath] = {}
        self._stage_artifacts: Dict[str, StageArtifacts] = {}
        self._workset_clusters: Dict[str, str] = {}
        self._work_yaml_cache: Dict[str, Dict[str, object]] = {}
        self._report_cache: Dict[str, WorksetReportMetrics] = {}

    # ------------------------------------------------------------------
    # Public entrypoints
    # ------------------------------------------------------------------
    def run(self) -> None:
        LOGGER.info("Starting Daylily workset monitor in %s", self.config.aws.region)
        if self.config.aws.session_duration_seconds:
            self._refresh_session()
        while True:
            start_time = time.time()
            try:
                worksets = list(self._discover_worksets())
                self._update_sentinel_indexes(worksets)
                for workset in worksets:
                    self._handle_workset(workset)
            except Exception:
                LOGGER.exception("Unexpected failure while monitoring worksets")
            elapsed = time.time() - start_time
            sleep_for = max(self.config.monitor.poll_interval_seconds - elapsed, 0)
            if not self.config.monitor.continuous:
                break
            if sleep_for:
                LOGGER.debug("Sleeping %.1fs before next poll", sleep_for)
                time.sleep(sleep_for)

    # ------------------------------------------------------------------
    # Workset discovery
    # ------------------------------------------------------------------
    def _discover_worksets(self) -> Iterable[Workset]:
        bucket = self.config.monitor.bucket
        prefix = self.config.monitor.normalised_prefix()
        paginator = self._s3.get_paginator("list_objects_v2")
        for page in paginator.paginate(Bucket=bucket, Prefix=prefix, Delimiter="/"):
            for common_prefix in page.get("CommonPrefixes", []):
                workset_prefix = common_prefix["Prefix"]
                name = workset_prefix.rstrip("/").split("/")[-1]
                sentinels = self._list_sentinels(workset_prefix)
                has_required = self._verify_core_files(workset_prefix)
                yield Workset(
                    name=name,
                    prefix=workset_prefix,
                    sentinels=sentinels,
                    has_required_files=has_required,
                )

    def _list_sentinels(self, workset_prefix: str) -> Dict[str, str]:
        """List all sentinel files for a workset with pagination safety."""
        bucket = self.config.monitor.bucket
        paginator = self._s3.get_paginator("list_objects_v2")
        sentinel_timestamps: Dict[str, str] = {}
        for page in paginator.paginate(Bucket=bucket, Prefix=workset_prefix):
            for obj in page.get("Contents", []) or []:
                key = obj["Key"]
                if not key.endswith(SENTINEL_SUFFIX):
                    continue
                name = key.split("/")[-1]
                with contextlib.suppress(KeyError):
                    sentinel_timestamps[name] = self._read_object_text(bucket, key)
        return sentinel_timestamps

    def _verify_core_files(self, workset_prefix: str) -> bool:
        bucket = self.config.monitor.bucket
        expected = [
            DEFAULT_STAGE_SAMPLES_NAME,
            WORK_YAML_NAME,
            INFO_YAML_NAME,
            SAMPLE_DATA_DIRNAME + "/",
        ]
        found = set()
        paginator = self._s3.get_paginator("list_objects_v2")
        for page in paginator.paginate(
            Bucket=bucket, Prefix=workset_prefix, Delimiter="/"
        ):
            for cp in page.get("CommonPrefixes", []) or []:
                if cp["Prefix"].endswith(SAMPLE_DATA_DIRNAME + "/"):
                    found.add(SAMPLE_DATA_DIRNAME + "/")
            for obj in page.get("Contents", []) or []:
                name = obj["Key"].split("/")[-1]
                if name in (DEFAULT_STAGE_SAMPLES_NAME, WORK_YAML_NAME, INFO_YAML_NAME):
                    found.add(name)
        missing = set(expected) - found
        if missing:
            LOGGER.warning(
                "Workset %s missing expected files: %s",
                workset_prefix,
                ", ".join(sorted(missing)),
            )
            return False
        return True

    # ------------------------------------------------------------------
    # Clone/run helpers (templating + FSx path fallback)
    # ------------------------------------------------------------------
    def _sanitize_name(self, s: str) -> str:
        return re.sub(r"[^A-Za-z0-9._-]", "_", s)

    def _workdir_name(self, workset: Workset, work_yaml: Dict[str, object]) -> str:
        wd = work_yaml.get("workdir_name")
        if isinstance(wd, str) and wd.strip():
            return self._sanitize_name(wd.strip())
        return self._sanitize_name(workset.name)

    def _format_clone_args(
        self, clone_args: str, workset: Workset, work_yaml: Dict[str, object]
    ) -> str:
        if not clone_args:
            return ""
        mapping = {
            "workset": workset.name,
            "workdir_name": self._workdir_name(workset, work_yaml),
        }
        try:
            return clone_args.format(**mapping)
        except Exception:
            return clone_args

    def _apply_budget_argument(
        self, clone_args: str, work_yaml: Dict[str, object]
    ) -> str:
        budget = work_yaml.get("budget")
        if budget is None:
            return clone_args
        if isinstance(budget, str):
            budget_value = budget.strip()
        else:
            budget_value = str(budget).strip()
        if not budget_value:
            return clone_args
        quoted = shlex.quote(budget_value)
        if clone_args.strip():
            return f"{clone_args.strip()} --budget {quoted}"
        return f"--budget {quoted}"

    def _extract_dest_from_clone_args(self, clone_args: str) -> Optional[str]:
        if not clone_args:
            return None
        parts = shlex.split(clone_args)
        for i, tok in enumerate(parts):
            if tok in ("-d", "--dest", "--destination"):
                if i + 1 < len(parts):
                    return self._sanitize_name(parts[i + 1])
            if tok.startswith("-d="):
                return self._sanitize_name(tok.split("=", 1)[1])
        return None

    def _expected_pipeline_dir(self, dest_name: str) -> PurePosixPath:
        root = self.config.pipeline.clone_dest_root.rstrip("/")
        repo = self.config.pipeline.repo_dir_name.strip("/")
        return PurePosixPath(f"{root}/{dest_name}/{repo}")

    # ------------------------------------------------------------------
    # Sentinel logging
    # ------------------------------------------------------------------
    def _update_sentinel_indexes(self, worksets: Sequence[Workset]) -> None:
        states: Dict[str, List[str]] = defaultdict(list)
        for workset in worksets:
            for sentinel, timestamp in workset.sentinels.items():
                if sentinel in OPTIONAL_SENTINELS or sentinel == SENTINEL_FILES["ready"]:
                    states[sentinel].append(f"{workset.name}\t{timestamp}")
        if not self.config.monitor.sentinel_index_prefix:
            return
        bucket = self.config.monitor.bucket
        base_prefix = self.config.monitor.sentinel_index_prefix
        base_prefix = base_prefix.rstrip("/") + "/" if base_prefix else ""
        for sentinel_name, rows in states.items():
            key = f"{base_prefix}{sentinel_name}.log"
            body = "\n".join(sorted(rows)).encode("utf-8")
            LOGGER.debug(
                "Updating sentinel index %s with %d entries", key, len(rows)
            )
            if self.dry_run:
                continue
            self._s3.put_object(Bucket=bucket, Key=key, Body=body)

    # ------------------------------------------------------------------
    # Workset state machine
    # ------------------------------------------------------------------
    def _handle_workset(self, workset: Workset) -> None:
        if not self._should_process(workset):
            LOGGER.info(
                "Skipping %s: not selected via --process-directory", workset.name
            )
            return
        sentinels = workset.sentinels
        if not sentinels:
            LOGGER.info("Skipping %s: no sentinel files present", workset.name)
            return
        if SENTINEL_FILES["ignore"] in sentinels:
            LOGGER.info("Skipping %s: daylily.ignore present", workset.name)
            return
        if SENTINEL_FILES["complete"] in sentinels:
            LOGGER.info(
                "Skipping %s: already complete (at %s)",
                workset.name,
                sentinels[SENTINEL_FILES["complete"]],
            )
            return
        if SENTINEL_FILES["error"] in sentinels:
            if self.attempt_restart:
                error_ts = sentinels[SENTINEL_FILES["error"]]
                LOGGER.info(
                    "Retrying %s: clearing error sentinel recorded at %s due to --attempt-restart",
                    workset.name,
                    error_ts,
                )
                self._delete_sentinel(workset, SENTINEL_FILES["error"])
                sentinels.pop(SENTINEL_FILES["error"], None)
                cleared: List[str] = []
                for stale in (SENTINEL_FILES["lock"], SENTINEL_FILES["in_progress"]):
                    if stale in sentinels:
                        self._delete_sentinel(workset, stale)
                        sentinels.pop(stale, None)
                        cleared.append(stale)
                if cleared:
                    LOGGER.debug(
                        "Removed stale sentinels for %s during restart: %s",
                        workset.name,
                        ", ".join(sorted(cleared)),
                    )
            else:
                LOGGER.info(
                    "Skipping %s: previously errored at %s",
                    workset.name,
                    sentinels[SENTINEL_FILES["error"]],
                )
                return
        if SENTINEL_FILES["in_progress"] in sentinels:
            LOGGER.info(
                "Skipping %s: currently marked in-progress (since %s)",
                workset.name,
                sentinels[SENTINEL_FILES["in_progress"]],
            )
            return
        if SENTINEL_FILES["ready"] not in sentinels:
            LOGGER.info("Skipping %s: ready sentinel missing", workset.name)
            return
        if not workset.has_required_files:
            LOGGER.warning("Skipping %s: required files missing", workset.name)
            return
        LOGGER.info("Attempting to acquire ready workset %s", workset.name)
        acquired = self._attempt_acquire(workset)
        if not acquired:
            LOGGER.info(
                "Workset %s lock attempt failed (contention or changed state)",
                workset.name,
            )
            return
        try:
            self._process_workset(workset)
        except Exception as exc:
            LOGGER.exception("Processing of %s failed", workset.name)
            self._write_sentinel(
                workset,
                SENTINEL_FILES["error"],
                f"{dt.datetime.utcnow().isoformat()}Z\t{exc}",
            )
        else:
            self._write_sentinel(
                workset,
                SENTINEL_FILES["complete"],
                f"{dt.datetime.utcnow().isoformat()}Z",
            )

    def _attempt_acquire(self, workset: Workset) -> bool:
        initial_snapshot = dict(workset.sentinels)
        timestamp = f"{dt.datetime.utcnow().isoformat()}Z"
        self._write_sentinel(workset, SENTINEL_FILES["lock"], timestamp)
        LOGGER.debug("Wrote lock sentinel for %s", workset.name)
        time.sleep(self.config.monitor.ready_lock_backoff_seconds)
        refreshed = self._list_sentinels(workset.prefix)
        unexpected = set(refreshed) - set(initial_snapshot)
        # If anything else changed besides our lock, treat as contention and back off (no error).
        if unexpected - {SENTINEL_FILES["lock"]}:
            LOGGER.warning(
                "Detected competing sentinel update for %s: %s",
                workset.name,
                ", ".join(sorted(unexpected)),
            )
            self._delete_sentinel(workset, SENTINEL_FILES["lock"])
            return False
        LOGGER.info("Acquired workset %s", workset.name)
        self._write_sentinel(
            workset,
            SENTINEL_FILES["in_progress"],
            f"{dt.datetime.utcnow().isoformat()}Z",
        )
        return True

    # ------------------------------------------------------------------
    # Workset processing pipeline
    # ------------------------------------------------------------------
    def _process_workset(self, workset: Workset) -> None:
        manifest_bytes = self._read_required_object(
            workset.prefix, DEFAULT_STAGE_SAMPLES_NAME
        )
        manifest_path = self._write_temp_file(
            workset, DEFAULT_STAGE_SAMPLES_NAME, manifest_bytes
        )
        manifest_path = self._copy_manifest_to_local(workset, manifest_path)
        work_yaml_bytes = self._read_required_object(workset.prefix, WORK_YAML_NAME)
        work_yaml = yaml.safe_load(work_yaml_bytes.decode("utf-8"))

        self._validate_stage_manifest(manifest_bytes, workset)

        clone_args_raw = self._yaml_get_str(
            work_yaml,
            ["day_clone_args", "day-clone", "clone_args", "clone-args", "clone"]
        ) or ""
        clone_args = self._format_clone_args(clone_args_raw, workset, work_yaml)
        clone_args = self._apply_budget_argument(clone_args, work_yaml)

        run_suffix = self._yaml_get_str(
            work_yaml,
            ["dy_r", "dy-r", "dy", "run", "run_suffix", "run-suffix", "run_cmd", "run-command"],
        )

        target_export_uri = self._yaml_get_str(
            work_yaml, ["export_uri", "export-uri", "export"]
        )

        cluster_name = self._ensure_cluster(work_yaml)
        self._record_workset_cluster(workset, cluster_name)
        LOGGER.info("Using cluster %s for workset %s", cluster_name, workset.name)

        completed_commands: Set[str] = set()
        if clone_args:
            pipeline_dir: Optional[PurePosixPath] = None
        else:
            # No clone; ensure fallback dir on headnode
            pipeline_dir = self._prepare_pipeline_workspace(
                workset, cluster_name, clone_args, run_clone=False
            )

        retry_attempted = False
        while True:
            try:
                pipeline_dir = self._execute_workset_commands(
                    workset,
                    manifest_path,
                    cluster_name,
                    clone_args,
                    run_suffix,
                    target_export_uri,
                    pipeline_dir,
                    completed_commands,
                )
                break
            except CommandFailedError as exc:
                if not self.attempt_restart or retry_attempted:
                    raise
                retry_attempted = True
                LOGGER.warning(
                    "Command %s failed for %s; attempting restart from this command",
                    exc.command_label,
                    workset.name,
                )
                continue

    def _execute_workset_commands(
        self,
        workset: Workset,
        manifest_path: Path,
        cluster_name: str,
        clone_args: str,
        run_suffix: Optional[str],
        target_export_uri: Optional[str],
        pipeline_dir: Optional[PurePosixPath],
        completed_commands: Set[str],
    ) -> PurePosixPath:
        stage_label = "stage_samples"
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            if stage_label in completed_commands:
                LOGGER.info(
                    "Skipping sample staging for %s: command already completed",
                    workset.name,
                )
                stage_future = executor.submit(
                    lambda: self._stage_artifacts.get(workset.name)
                )
            else:
                stage_future = executor.submit(
                    self._stage_samples, workset, manifest_path, cluster_name
                )
            cluster_future = executor.submit(
                self._wait_for_cluster_ready, cluster_name
            )
            cluster_future.result()
            try:
                stage_artifacts = stage_future.result()
            except CommandFailedError:
                raise
        if stage_artifacts is None:
            stage_artifacts = self._stage_artifacts.get(workset.name)
        if stage_artifacts is None:
            raise MonitorError(
                f"Stage samples output unavailable for {workset.name}; rerun staging"
            )
        completed_commands.add(stage_label)

        clone_label = "clone_pipeline"
        clone_needed = bool(clone_args) and clone_label not in completed_commands
        if clone_needed:
            LOGGER.info("Running pipeline clone for %s", workset.name)
            pipeline_dir = self._prepare_pipeline_workspace(
                workset, cluster_name, clone_args, run_clone=True
            )
            completed_commands.add(clone_label)
        elif clone_args:
            LOGGER.info(
                "Skipping pipeline clone for %s: command already completed",
                workset.name,
            )
            if pipeline_dir is None:
                pipeline_dir = self._prepare_pipeline_workspace(
                    workset, cluster_name, clone_args, run_clone=False
                )

        if pipeline_dir is None:
            raise MonitorError(
                "Pipeline directory unavailable; rerun day-clone or remove cached state"
            )

        push_label = "push_stage_files"
        if push_label in completed_commands:
            LOGGER.info(
                "Skipping stage file push for %s: command already completed",
                workset.name,
            )
        else:
            self._push_stage_files_to_pipeline(
                cluster_name, pipeline_dir, manifest_path, stage_artifacts
            )
            completed_commands.add(push_label)

        run_label = "run_pipeline"
        if run_label in completed_commands:
            LOGGER.info(
                "Skipping pipeline run for %s: command already completed",
                workset.name,
            )
        else:
            self._run_pipeline(workset, cluster_name, pipeline_dir, run_suffix)
            completed_commands.add(run_label)

        if target_export_uri:
            export_label = "export_results"
            if export_label in completed_commands:
                LOGGER.info(
                    "Skipping export for %s: command already completed", workset.name
                )
            else:
                self._export_results(workset, cluster_name, target_export_uri)
                completed_commands.add(export_label)

        return pipeline_dir

    def _local_state_dir(self, workset: Workset) -> Path:
        root = self.config.pipeline.local_state_root or "~/.cache/daylily-monitor"
        path = Path(os.path.expanduser(root)) / workset.name
        path.mkdir(parents=True, exist_ok=True)
        return path

    def _cluster_state_dir(self, cluster_name: str) -> Path:
        root = self.config.pipeline.local_state_root or "~/.cache/daylily-monitor"
        path = Path(os.path.expanduser(root)) / CLUSTER_STATE_DIR / cluster_name
        path.mkdir(parents=True, exist_ok=True)
        return path

    def _cluster_idle_marker(self, cluster_name: str) -> Path:
        return self._cluster_state_dir(cluster_name) / CLUSTER_IDLE_MARKER

    def _clear_cluster_idle(self, cluster_name: str) -> None:
        with contextlib.suppress(FileNotFoundError):
            self._cluster_idle_marker(cluster_name).unlink()

    def _record_pipeline_location(
        self, workset: Workset, location: PurePosixPath
    ) -> None:
        self._pipeline_locations[workset.name] = location
        state_dir = self._local_state_dir(workset)
        marker = state_dir / PIPELINE_LOCATION_MARKER
        marker.write_text(str(location), encoding="utf-8")

    def _load_pipeline_location(self, workset: Workset) -> Optional[PurePosixPath]:
        cached = self._pipeline_locations.get(workset.name)
        if cached:
            return cached
        marker = self._local_state_dir(workset) / PIPELINE_LOCATION_MARKER
        if not marker.exists():
            return None
        text = marker.read_text(encoding="utf-8").strip()
        if not text:
            return None
        location = PurePosixPath(text)
        self._pipeline_locations[workset.name] = location
        return location

    def _record_workset_cluster(self, workset: Workset, cluster_name: str) -> None:
        self._workset_clusters[workset.name] = cluster_name
        marker = self._local_state_dir(workset) / WORKSET_CLUSTER_MARKER
        marker.write_text(cluster_name, encoding="utf-8")

    def _load_workset_cluster(self, workset: Workset) -> Optional[str]:
        cached = self._workset_clusters.get(workset.name)
        if cached:
            return cached
        marker = self._local_state_dir(workset) / WORKSET_CLUSTER_MARKER
        if not marker.exists():
            return None
        text = marker.read_text(encoding="utf-8").strip()
        if not text:
            return None
        self._workset_clusters[workset.name] = text
        return text

    def _report_cache_path(self, workset: Workset) -> Path:
        return self._local_state_dir(workset) / REPORT_CACHE_FILENAME

    def _load_report_cache(self, workset: Workset) -> Optional[WorksetReportMetrics]:
        cached = self._report_cache.get(workset.name)
        if cached:
            return cached
        path = self._report_cache_path(workset)
        if not path.exists():
            return None
        try:
            raw = json.loads(path.read_text(encoding="utf-8"))
        except (OSError, json.JSONDecodeError):
            return None
        if not isinstance(raw, dict):
            return None
        metrics = WorksetReportMetrics(
            cluster_name=raw.get("cluster_name"),
            region_az=raw.get("region_az"),
            budget_name=raw.get("budget_name"),
            run_seconds=raw.get("run_seconds"),
            end_dt=self._parse_timestamp(raw.get("end_dt")),
            pipeline_command=raw.get("pipeline_command"),
            clone_command=raw.get("clone_command"),
            samples=raw.get("samples"),
            sample_libraries=raw.get("sample_libraries"),
            fastq_files=raw.get("fastq_files"),
            fastq_bytes=raw.get("fastq_bytes"),
            cram_files=raw.get("cram_files"),
            cram_bytes=raw.get("cram_bytes"),
            vcf_files=raw.get("vcf_files"),
            vcf_bytes=raw.get("vcf_bytes"),
            results_bytes=raw.get("results_bytes"),
            s3_daily_cost=raw.get("s3_daily_cost"),
            cram_region_cost=raw.get("cram_region_cost"),
            cram_internet_cost=raw.get("cram_internet_cost"),
            vcf_region_cost=raw.get("vcf_region_cost"),
            vcf_internet_cost=raw.get("vcf_internet_cost"),
            ec2_cost=raw.get("ec2_cost"),
        )
        self._report_cache[workset.name] = metrics
        return metrics

    def _save_report_cache(self, workset: Workset, metrics: WorksetReportMetrics) -> None:
        path = self._report_cache_path(workset)
        try:
            path.write_text(json.dumps(metrics.as_dict()), encoding="utf-8")
        except OSError:
            LOGGER.debug("Unable to persist report cache for %s", workset.name)
        else:
            self._report_cache[workset.name] = metrics

    def _parse_day_clone_location(self, output: bytes) -> Optional[PurePosixPath]:
        text = output.decode("utf-8", errors="ignore")
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if stripped.lower().startswith("location"):
                _, _, remainder = stripped.partition(":")
                candidate = remainder.strip()
                if candidate:
                    return PurePosixPath(candidate)
            if stripped.startswith("cd "):
                parts = shlex.split(stripped)
                if len(parts) >= 2:
                    return PurePosixPath(parts[1])
        return None

    def _record_tmux_session(self, workset: Workset, session_name: str) -> None:
        state_dir = self._local_state_dir(workset)
        marker = state_dir / PIPELINE_SESSION_MARKER
        marker.write_text(session_name, encoding="utf-8")

    def _generate_tmux_session_name(self, workset: Workset) -> str:
        prefix = self.config.pipeline.tmux_session_prefix or "daylily"
        safe_prefix = re.sub(r"[^A-Za-z0-9_-]", "_", prefix)
        safe_name = re.sub(r"[^A-Za-z0-9_-]", "_", workset.name)
        timestamp = int(time.time())
        return f"{safe_prefix}_{safe_name}_{timestamp}"

    def _load_tmux_session(self, workset: Workset) -> Optional[str]:
        marker = self._local_state_dir(workset) / PIPELINE_SESSION_MARKER
        if not marker.exists():
            return None
        session_name = marker.read_text(encoding="utf-8").strip()
        return session_name or None

    def _clear_tmux_session(self, workset: Workset) -> None:
        marker = self._local_state_dir(workset) / PIPELINE_SESSION_MARKER
        with contextlib.suppress(FileNotFoundError):
            marker.unlink()

    def _pipeline_start_marker(self, workset: Workset) -> Path:
        return self._local_state_dir(workset) / PIPELINE_START_MARKER

    def _record_pipeline_start(self, workset: Workset, timestamp: dt.datetime) -> None:
        marker = self._pipeline_start_marker(workset)
        marker.write_text(timestamp.isoformat(), encoding="utf-8")

    def _load_pipeline_start(self, workset: Workset) -> Optional[dt.datetime]:
        marker = self._pipeline_start_marker(workset)
        if not marker.exists():
            return None
        text = marker.read_text(encoding="utf-8").strip()
        if not text:
            return None
        with contextlib.suppress(ValueError):
            return dt.datetime.fromisoformat(text)
        return None

    def _clear_pipeline_start(self, workset: Workset) -> None:
        with contextlib.suppress(FileNotFoundError):
            self._pipeline_start_marker(workset).unlink()

    def _validate_stage_manifest(
        self, manifest_bytes: bytes, workset: Workset
    ) -> None:
        lines = manifest_bytes.decode("utf-8").splitlines()
        if not lines:
            raise MonitorError(f"stage_samples.tsv for {workset.name} is empty")
        header = lines[0].split("\t")
        s3_columns = [
            idx
            for idx, name in enumerate(header)
            if name.lower().endswith("_uri") or name.lower().startswith("s3")
        ]
        sample_data_columns = [
            idx
            for idx, name in enumerate(header)
            if name.lower().startswith("path") or name.lower().endswith("_path")
        ]
        for line in lines[1:]:
            if not line.strip():
                continue
            cells = line.split("\t")
            for idx in s3_columns:
                if idx >= len(cells):
                    continue
                value = cells[idx].strip()
                if value:
                    self._assert_s3_uri_exists(value)
            for idx in sample_data_columns:
                if idx >= len(cells):
                    continue
                value = cells[idx].strip()
                if value:
                    self._assert_sample_file_exists(workset, value)

    def _stage_samples(
        self, workset: Workset, manifest_path: Path, cluster_name: str
    ) -> StageArtifacts:
        manifest_argument = self._relative_manifest_argument(manifest_path)
        reference_bucket = self._stage_reference_bucket()
        cmd = self.config.pipeline.stage_command.format(
            profile=self.config.aws.profile,
            region=self.config.aws.region,
            cluster=cluster_name,
            analysis_samples=manifest_argument,
            reference_bucket=reference_bucket,
            ssh_identity_file=self.config.pipeline.ssh_identity_file or "",
            pem=self.config.pipeline.ssh_identity_file or "",
            ssh_user=self.config.pipeline.ssh_user,
            ssh_extra_args=" ".join(
                shlex.quote(arg) for arg in self.config.pipeline.ssh_extra_args
            ),
        )
        LOGGER.info(
            "Staging samples for cluster %s with command: %s", cluster_name, cmd
        )
        result = self._run_monitored_command("stage_samples", cmd, check=True)
        artifacts = self._parse_stage_samples_output(result)
        self._stage_artifacts[workset.name] = artifacts
        LOGGER.info(
            "Recorded %d staged files for %s under %s",
            len(artifacts.staged_files),
            workset.name,
            artifacts.fsx_stage_dir,
        )
        return artifacts

    def _parse_stage_samples_output(
        self, result: subprocess.CompletedProcess
    ) -> StageArtifacts:
        outputs: List[str] = []
        if isinstance(result.stdout, (bytes, bytearray)):
            outputs.append(result.stdout.decode("utf-8", errors="ignore"))
        elif isinstance(result.stdout, str):
            outputs.append(result.stdout)
        if isinstance(result.stderr, (bytes, bytearray)):
            outputs.append(result.stderr.decode("utf-8", errors="ignore"))
        elif isinstance(result.stderr, str):
            outputs.append(result.stderr)

        combined = "\n".join(filter(None, outputs)).strip()
        if not combined:
            raise MonitorError("Stage command produced no output to parse")

        fsx_stage_dir: Optional[PurePosixPath] = None
        staged_files: List[PurePosixPath] = []
        collecting_files = False

        for raw_line in combined.splitlines():
            line = raw_line.strip()
            if not line:
                if collecting_files:
                    collecting_files = False
                continue

            if line.startswith("Remote FSx stage directory:"):
                _, value = raw_line.split(":", 1)
                path_str = value.strip()
                if not path_str:
                    raise MonitorError(
                        "Stage command did not report an FSx stage directory"
                    )
                fsx_stage_dir = PurePosixPath("/fsx") / path_str.lstrip("/")
                continue

            if line.startswith("Staged files"):
                collecting_files = True
                continue

            if collecting_files:
                if raw_line.startswith(" ") or raw_line.startswith("\t"):
                    path_candidate = line
                    if path_candidate:
                        fsx_path = PurePosixPath("/fsx") / path_candidate.lstrip("/")
                        staged_files.append(fsx_path)
                    continue
                collecting_files = False

        if fsx_stage_dir is None:
            raise MonitorError(
                "Unable to determine remote FSx stage directory from stage command output"
            )

        # Remove duplicates while preserving order
        unique_files: List[PurePosixPath] = []
        seen: Set[PurePosixPath] = set()
        for path in staged_files:
            if path not in seen:
                unique_files.append(path)
                seen.add(path)

        stage_dir_name = fsx_stage_dir.name
        timestamp = stage_dir_name.replace("remote_stage_", "")
        samples_manifest = fsx_stage_dir / f"{timestamp}_samples.tsv"
        units_manifest = fsx_stage_dir / f"{timestamp}_units.tsv"

        return StageArtifacts(
            fsx_stage_dir=fsx_stage_dir,
            staged_files=unique_files,
            samples_manifest=samples_manifest,
            units_manifest=units_manifest,
        )

    def _wait_for_cluster_ready(self, cluster_name: str) -> None:
        LOGGER.info("Waiting for cluster %s to become ready", cluster_name)
        for attempt in range(60):
            details = self._describe_cluster(cluster_name)
            if details and self._cluster_is_ready(details):
                LOGGER.debug(
                    "Cluster %s ready (checked %d times)", cluster_name, attempt + 1
                )
                return
            LOGGER.debug("Cluster %s not ready yet (%d)", cluster_name, attempt + 1)
            time.sleep(30)
        raise MonitorError(f"Cluster {cluster_name} did not become ready in time")

    def _prepare_pipeline_workspace(
        self,
        workset: Workset,
        cluster_name: str,
        clone_args: str,
        *,
        run_clone: bool = True,
    ) -> PurePosixPath:
        if clone_args and run_clone:
            init = (self.config.pipeline.login_shell_init or "").strip()
            base = self.config.pipeline.clone_command.format(clone_args=clone_args)
            cmd = f"{init} && {base}" if init else base
            result = self._run_headnode_monitored_command(
                "clone_pipeline",
                cmd,
                cluster_name=cluster_name,
                check=True,
                shell=True,  # pass as raw string to bash -lc
            )
            location = self._parse_day_clone_location(result.stdout)
            if not location and result.stderr:
                location = self._parse_day_clone_location(result.stderr)
            if not location:
                dest = self._extract_dest_from_clone_args(clone_args) or self._sanitize_name(workset.name)
                location = self._expected_pipeline_dir(dest)
                LOGGER.info(
                    "day-clone did not report Location; falling back to %s", location
                )
            LOGGER.info(
                "day-clone reported pipeline directory %s for %s",
                location,
                workset.name,
            )
            self._record_pipeline_location(workset, location)
            return location
        if clone_args:
            location = self._load_pipeline_location(workset)
            if location:
                LOGGER.info(
                    "Reusing recorded pipeline directory %s for %s",
                    location,
                    workset.name,
                )
                return location
            raise MonitorError(
                "Pipeline location unavailable for restart; rerun day-clone or remove cached state"
            )
        # No clone requested; ensure fallback dir on headnode exists
        fallback = PurePosixPath(self.config.pipeline.workdir) / workset.name
        ensure_cmd = f"mkdir -p {shlex.quote(str(fallback))}"
        self._run_headnode_command(cluster_name, ensure_cmd, check=True, shell=True)
        self._record_pipeline_location(workset, fallback)
        return fallback

    def _push_stage_files_to_pipeline(
        self,
        cluster_name: str,
        pipeline_dir: PurePosixPath,
        manifest_path: Path,
        stage_artifacts: StageArtifacts,
    ) -> None:
        config_dir = pipeline_dir / "config"
        mkdir_cmd = f"mkdir -p {shlex.quote(str(config_dir))}"
        self._run_headnode_monitored_command(
            "push_stage_files",
            mkdir_cmd,
            cluster_name=cluster_name,
            check=True,
            shell=True,
        )
        LOGGER.info("Copying staged artifacts into %s", pipeline_dir)
        self._copy_stage_artifacts_to_pipeline(
            cluster_name, pipeline_dir, stage_artifacts, manifest_path
        )

    def _copy_stage_artifacts_to_pipeline(
        self,
        cluster_name: str,
        pipeline_dir: PurePosixPath,
        stage_artifacts: StageArtifacts,
        manifest_path: Path,
    ) -> None:
        sample_data_root = pipeline_dir / SAMPLE_DATA_DIRNAME
        mkdir_data = f"mkdir -p {shlex.quote(str(sample_data_root))}"
        self._run_headnode_monitored_command(
            "push_stage_files",
            mkdir_data,
            cluster_name=cluster_name,
            check=True,
            shell=True,
        )

        for staged_file in stage_artifacts.staged_files:
            try:
                relative_path = staged_file.relative_to(stage_artifacts.fsx_stage_dir)
            except ValueError:
                LOGGER.warning(
                    "Staged file %s is not within %s; skipping copy",
                    staged_file,
                    stage_artifacts.fsx_stage_dir,
                )
                continue
            destination = sample_data_root / relative_path
            copy_cmd = (
                f"mkdir -p {shlex.quote(str(destination.parent))} && "
                f"cp -p {shlex.quote(str(staged_file))} {shlex.quote(str(destination))}"
            )
            self._run_headnode_monitored_command(
                "push_stage_files",
                copy_cmd,
                cluster_name=cluster_name,
                check=True,
                shell=True,
            )

        config_dir = pipeline_dir / "config"
        samples_target = config_dir / "samples.tsv"
        units_target = config_dir / "units.tsv"

        copied_samples = False
        if stage_artifacts.samples_manifest:
            copied_samples = self._copy_file_on_headnode(
                cluster_name,
                stage_artifacts.samples_manifest,
                samples_target,
                label="push_stage_files",
                check=False,
            )
        copied_units = False
        if stage_artifacts.units_manifest:
            copied_units = self._copy_file_on_headnode(
                cluster_name,
                stage_artifacts.units_manifest,
                units_target,
                label="push_stage_files",
                check=False,
            )

        if not copied_samples:
            LOGGER.info(
                "Falling back to SCP for samples manifest from %s", manifest_path
            )
            scp_samples = self._build_scp_command(
                cluster_name, manifest_path, samples_target
            )
            self._run_monitored_command("push_stage_files", scp_samples, check=True)

        units_src = manifest_path.with_name("units.tsv")
        need_units_fallback = (
            (stage_artifacts.units_manifest is None or not copied_units)
            and units_src.exists()
        )
        if need_units_fallback:
            LOGGER.info(
                "Falling back to SCP for units manifest from %s", units_src
            )
            scp_units = self._build_scp_command(
                cluster_name, units_src, units_target
            )
            self._run_monitored_command("push_stage_files", scp_units, check=True)
        elif not copied_units and stage_artifacts.units_manifest is not None:
            LOGGER.warning(
                "Units manifest %s was not copied to %s and no fallback was available",
                stage_artifacts.units_manifest,
                units_target,
            )
        elif stage_artifacts.units_manifest is None and not units_src.exists():
            LOGGER.warning(
                "Units manifest missing: expected local fallback at %s",
                units_src,
            )

    def _copy_file_on_headnode(
        self,
        cluster_name: str,
        source: PurePosixPath,
        destination: PurePosixPath,
        *,
        label: str,
        check: bool,
    ) -> bool:
        quoted_source = shlex.quote(str(source))
        quoted_dest = shlex.quote(str(destination))
        quoted_parent = shlex.quote(str(destination.parent))
        command = (
            f"mkdir -p {quoted_parent} && cp -p {quoted_source} {quoted_dest}"
        )
        result = self._run_headnode_monitored_command(
            label,
            command,
            cluster_name=cluster_name,
            check=check,
            shell=True,
        )
        if check:
            return True
        if result.returncode != 0:
            LOGGER.debug(
                "Headnode copy command exited %s for %s -> %s",
                result.returncode,
                source,
                destination,
            )
            return False
        return True

    def _headnode_path_exists(
        self, cluster_name: str, path: PurePosixPath
    ) -> bool:
        cmd = f"test -e {shlex.quote(str(path))}"
        result = self._run_headnode_command(
            cluster_name, cmd, check=False, shell=True
        )
        return result.returncode == 0

    def _pipeline_sentinel_status(
        self, cluster_name: str, pipeline_dir: PurePosixPath
    ) -> str:
        success = pipeline_dir / PIPELINE_SUCCESS_SENTINEL
        failure = pipeline_dir / PIPELINE_FAILURE_SENTINEL
        if self._headnode_path_exists(cluster_name, success):
            return "success"
        if self._headnode_path_exists(cluster_name, failure):
            return "failure"
        return "pending"

    def _tmux_session_exists(self, cluster_name: str, session_name: str) -> bool:
        result = self._run_headnode_command(
            cluster_name,
            ["tmux", "has-session", "-t", session_name],
            check=False,
            shell=False,
        )
        return result.returncode == 0

    def _terminate_tmux_session(
        self, cluster_name: str, session_name: Optional[str]
    ) -> None:
        if not session_name:
            return
        self._run_headnode_command(
            cluster_name,
            ["tmux", "kill-session", "-t", session_name],
            check=False,
            shell=False,
        )

    def _interrupt_tmux_session(
        self, cluster_name: str, session_name: Optional[str]
    ) -> None:
        if not session_name:
            return
        self._run_headnode_command(
            cluster_name,
            ["tmux", "send-keys", "-t", session_name, "C-x"],
            check=False,
            shell=False,
        )

    def _run_pipeline(
        self,
        workset: Workset,
        cluster_name: str,
        pipeline_dir: PurePosixPath,
        run_suffix: Optional[str],
    ) -> None:
        if not run_suffix:
            raise MonitorError(
                "Missing pipeline run suffix in daylily_work.yaml. "
                "Provide one of: dy-r, dy_r, dy, run, run_suffix, run-suffix, run_cmd."
            )

        status = self._pipeline_sentinel_status(cluster_name, pipeline_dir)
        existing_session = self._load_tmux_session(workset)
        if status == "success":
            LOGGER.info(
                "Pipeline already marked successful for %s; skipping rerun", workset.name
            )
            self._terminate_tmux_session(cluster_name, existing_session)
            self._clear_tmux_session(workset)
            self._clear_pipeline_start(workset)
            return
        if status == "failure":
            self._terminate_tmux_session(cluster_name, existing_session)
            self._clear_tmux_session(workset)
            self._clear_pipeline_start(workset)
            raise MonitorError(
                f"Pipeline previously failed for {workset.name}; investigate {PIPELINE_FAILURE_SENTINEL}"
            )

        if existing_session and not self._tmux_session_exists(
            cluster_name, existing_session
        ):
            self._clear_tmux_session(workset)
            self._clear_pipeline_start(workset)
            raise MonitorError(
                f"Recorded tmux session {existing_session} for {workset.name} is not running and no sentinel was found"
            )

        session_name = existing_session
        if not session_name:
            run_command = self.config.pipeline.run_prefix + run_suffix

            steps: List[str] = []
            steps.append("echo go")
            steps.append(f"cd {shlex.quote(str(pipeline_dir))}")

            init_cmd = (self.config.pipeline.login_shell_init or "").strip()
            if init_cmd:
                steps.append(init_cmd)

            steps.append(run_command)

            keepalive = (self.config.pipeline.tmux_keepalive_shell or "").strip()
            if keepalive:
                steps.append(keepalive)

            composite = " && ".join(steps)

            session_name = self._generate_tmux_session_name(workset)
            tmux_cmd = [
                "tmux",
                "new-session",
                "-d",
                "-s",
                session_name,
                "bash",
                "-lc",
                composite,
            ]

            LOGGER.info(
                "Launching pipeline for %s in tmux session %s: %s",
                workset.name,
                session_name,
                run_command,
            )
            self._record_tmux_session(workset, session_name)
            self._write_pipeline_sentinel(cluster_name, pipeline_dir, "START")
            try:
                self._run_headnode_monitored_command(
                    "run_pipeline",
                    tmux_cmd,
                    cluster_name=cluster_name,
                    check=True,
                    shell=False,
                )
                self._run_headnode_command(
                    cluster_name,
                    ["tmux", "has-session", "-t", session_name],
                    check=True,
                    shell=False,
                )
                self._record_pipeline_start(workset, dt.datetime.utcnow())
            except Exception:
                self._clear_tmux_session(workset)
                self._clear_pipeline_start(workset)
                raise
            finally:
                self._write_pipeline_sentinel(cluster_name, pipeline_dir, "END")

        self._monitor_pipeline_session(
            workset, cluster_name, pipeline_dir, session_name
        )

    def _monitor_pipeline_session(
        self,
        workset: Workset,
        cluster_name: str,
        pipeline_dir: PurePosixPath,
        session_name: str,
    ) -> None:
        poll_interval = max(10, min(60, self.config.monitor.poll_interval_seconds))
        start_time = self._load_pipeline_start(workset)
        if start_time is None:
            start_time = dt.datetime.utcnow()
            self._record_pipeline_start(workset, start_time)
        timeout_minutes = self.config.pipeline.pipeline_timeout_minutes
        timeout_seconds = (timeout_minutes or 0) * 60 if timeout_minutes else None

        while True:
            status = self._pipeline_sentinel_status(cluster_name, pipeline_dir)
            if status == "success":
                LOGGER.info("Pipeline completed successfully for %s", workset.name)
                self._terminate_tmux_session(cluster_name, session_name)
                self._clear_tmux_session(workset)
                self._clear_pipeline_start(workset)
                return
            if status == "failure":
                self._terminate_tmux_session(cluster_name, session_name)
                self._clear_tmux_session(workset)
                self._clear_pipeline_start(workset)
                raise MonitorError(
                    f"Pipeline failed for {workset.name}; see {PIPELINE_FAILURE_SENTINEL}"
                )

            if not self._tmux_session_exists(cluster_name, session_name):
                self._clear_tmux_session(workset)
                self._clear_pipeline_start(workset)
                raise MonitorError(
                    f"Pipeline session {session_name} exited without a sentinel for {workset.name}"
                )

            if timeout_seconds is not None:
                elapsed = (dt.datetime.utcnow() - start_time).total_seconds()
                if elapsed >= timeout_seconds:
                    LOGGER.error(
                        "Pipeline for %s exceeded timeout of %d minutes",
                        workset.name,
                        timeout_minutes,
                    )
                    status = self._pipeline_sentinel_status(cluster_name, pipeline_dir)
                    if status == "success":
                        LOGGER.info(
                            "Pipeline for %s reported success sentinel during timeout handling",
                            workset.name,
                        )
                        continue
                    if status == "failure":
                        self._terminate_tmux_session(cluster_name, session_name)
                        self._clear_tmux_session(workset)
                        self._clear_pipeline_start(workset)
                        raise MonitorError(
                            f"Pipeline failed for {workset.name}; see {PIPELINE_FAILURE_SENTINEL}"
                        )
                    self._interrupt_tmux_session(cluster_name, session_name)
                    time.sleep(5)
                    self._terminate_tmux_session(cluster_name, session_name)
                    self._clear_tmux_session(workset)
                    self._clear_pipeline_start(workset)
                    raise MonitorError(
                        f"Pipeline timed out after {timeout_minutes} minutes for {workset.name}"
                    )

            time.sleep(poll_interval)

    def _write_pipeline_sentinel(
        self,
        cluster_name: str,
        pipeline_dir: PurePosixPath,
        state: str,
    ) -> None:
        """
        Append a one-line UTC timestamp + state to a local log on the headnode.
        Idempotent and safe under re-runs.
        """
        # Quote everything carefully; run on headnode.
        log_path = str(pipeline_dir / ".daylily-monitor-sentinel.log")
        cmd = (
            f'printf "%s\\t%s\\n" "$(date -u +%FT%TZ)" {shlex.quote(state)} '
            f'>> {shlex.quote(log_path)}'
        )
        self._run_headnode_monitored_command(
            "pipeline_sentinel",
            cmd,
            cluster_name=cluster_name,
            check=True,
            shell=True,
        )

    def _export_results(
        self, workset: Workset, cluster_name: str, target_uri: str
    ) -> None:
        output_dir = self._local_state_dir(workset)
        profile_value = self.config.aws.profile or ""
        command = self.config.pipeline.export_command.format(
            cluster=shlex.quote(cluster_name),
            target_uri=shlex.quote(target_uri),
            region=shlex.quote(self.config.aws.region),
            profile=shlex.quote(profile_value) if profile_value else "",
            output_dir=shlex.quote(str(output_dir)),
        )
        LOGGER.info("Exporting pipeline results for %s to %s", workset.name, target_uri)
        self._run_monitored_command(
            "export_results", command, check=True, shell=True
        )

        status_path = output_dir / FSX_EXPORT_STATUS_FILENAME
        if not status_path.exists():
            raise MonitorError(
                f"Export command for {workset.name} did not produce {FSX_EXPORT_STATUS_FILENAME}"
            )
        try:
            status_data = yaml.safe_load(status_path.read_text(encoding="utf-8"))
        except yaml.YAMLError as exc:
            raise MonitorError(
                f"Unable to parse export status for {workset.name}: {exc}"
            ) from exc
        if not isinstance(status_data, dict) or "fsx_export" not in status_data:
            raise MonitorError(
                f"Export status for {workset.name} missing fsx_export block"
            )
        details = status_data.get("fsx_export")
        if not isinstance(details, dict):
            raise MonitorError(
                f"Export status for {workset.name} has unexpected format"
            )
        status = str(details.get("status", "")).lower()
        s3_uri = details.get("s3_uri")
        if status != "success":
            raise MonitorError(
                f"Export command reported failure for {workset.name}: status={status or 'unknown'}"
            )
        LOGGER.info(
            "Export completed for %s; results available at %s", workset.name, s3_uri
        )
        if self.config.cluster.auto_teardown:
            self._maybe_shutdown_cluster(cluster_name)

    # ------------------------------------------------------------------
    # Cluster helpers
    # ------------------------------------------------------------------
    def _cluster_has_tmux_sessions(self, cluster_name: str) -> bool:
        result = self._run_headnode_command(
            cluster_name, ["tmux", "list-sessions"], check=False, shell=False
        )
        if result.returncode != 0:
            return False
        output = result.stdout
        if isinstance(output, (bytes, bytearray)):
            text = output.decode("utf-8", errors="ignore")
        else:
            text = str(output)
        return bool(text.strip())

    def _cluster_job_count(self, cluster_name: str) -> Optional[int]:
        cmd = "squeue | wc -l"
        result = self._run_headnode_command(
            cluster_name, ["bash", "-lc", cmd], check=False, shell=False
        )
        if result.returncode != 0:
            LOGGER.warning(
                "Unable to determine job count on %s (exit %s)",
                cluster_name,
                result.returncode,
            )
            return None
        output = result.stdout
        if isinstance(output, (bytes, bytearray)):
            text = output.decode("utf-8", errors="ignore")
        else:
            text = str(output)
        try:
            return int(text.strip() or "0")
        except ValueError:
            LOGGER.warning(
                "Unexpected job count output from %s: %s", cluster_name, text.strip()
            )
            return None

    def _shutdown_cluster(self, cluster_name: str) -> None:
        LOGGER.info("Shutting down cluster %s", cluster_name)
        cmd = [
            "pcluster",
            "delete-cluster",
            "--region",
            self.config.aws.region,
            "-n",
            cluster_name,
        ]
        self._run_monitored_command(
            "delete_cluster", cmd, check=True, env=self._pcluster_env()
        )

    def _maybe_shutdown_cluster(self, cluster_name: str) -> None:
        if not self.config.cluster.auto_teardown:
            return
        if self._cluster_has_tmux_sessions(cluster_name):
            LOGGER.info(
                "Skipping shutdown of %s: tmux sessions remain active", cluster_name
            )
            self._clear_cluster_idle(cluster_name)
            return
        job_count = self._cluster_job_count(cluster_name)
        if job_count is None:
            LOGGER.info(
                "Skipping shutdown of %s: unable to determine job queue", cluster_name
            )
            self._clear_cluster_idle(cluster_name)
            return
        if job_count > 1:
            LOGGER.info(
                "Skipping shutdown of %s: %d jobs still in queue", cluster_name, job_count - 1
            )
            self._clear_cluster_idle(cluster_name)
            return

        idle_marker = self._cluster_idle_marker(cluster_name)
        now = dt.datetime.utcnow()
        if idle_marker.exists():
            text = idle_marker.read_text(encoding="utf-8").strip()
            with contextlib.suppress(ValueError):
                recorded = dt.datetime.fromisoformat(text)
                elapsed = (now - recorded).total_seconds()
                if elapsed >= 600:
                    LOGGER.info(
                        "Cluster %s idle for %.0f seconds; initiating shutdown",
                        cluster_name,
                        elapsed,
                    )
                    self._shutdown_cluster(cluster_name)
                    self._clear_cluster_idle(cluster_name)
                    return
                LOGGER.info(
                    "Cluster %s idle for %.0f seconds; waiting for 600 seconds",
                    cluster_name,
                    elapsed,
                )
                return
        idle_marker.write_text(now.isoformat(), encoding="utf-8")
        LOGGER.info(
            "Cluster %s is idle; will shutdown after 600 seconds if still idle",
            cluster_name,
        )

    def _ensure_cluster(self, work_yaml: Dict[str, object]) -> str:
        if self.config.cluster.reuse_cluster_name:
            return self.config.cluster.reuse_cluster_name
        existing = self._find_existing_cluster()
        if existing:
            return existing
        return self._create_cluster(work_yaml)

    def _pcluster_env(self) -> Dict[str, str]:
        env = os.environ.copy()
        if self.config.aws.profile:
            env["AWS_PROFILE"] = self.config.aws.profile
        return env

    def _load_pcluster_json(self, raw: bytes) -> Optional[object]:
        text = raw.decode("utf-8", errors="ignore").strip()
        if not text:
            return None
        brace_positions = [pos for pos in (text.find("{"), text.find("[")) if pos != -1]
        if brace_positions:
            text = text[min(brace_positions):]
        try:
            return json.loads(text)
        except json.JSONDecodeError as exc:
            LOGGER.debug("Failed to decode pcluster output as JSON: %s", exc)
            return None

    def _describe_cluster(self, cluster_name: str) -> Optional[Dict[str, object]]:
        cmd = ["pcluster", "describe-cluster", "--region", self.config.aws.region, "-n", cluster_name]
        result = self._run_command(cmd, check=False, env=self._pcluster_env())
        if result.returncode != 0:
            LOGGER.debug(
                "Unable to describe cluster %s (exit %s): %s",
                cluster_name,
                result.returncode,
                result.stderr.decode(errors="ignore"),
            )
            return None
        payload = self._load_pcluster_json(result.stdout)
        if isinstance(payload, dict):
            return payload
        LOGGER.debug("Unexpected describe-cluster output for %s", cluster_name)
        return None

    def _cluster_is_ready(self, details: Dict[str, object]) -> bool:
        cluster_status = str(details.get("clusterStatus", "")).upper()
        compute_status = str(details.get("computeFleetStatus", "")).upper()
        if cluster_status not in READY_CLUSTER_STATUSES:
            return False
        if compute_status and compute_status not in READY_COMPUTE_FLEET_STATUSES:
            return False
        return True

    def _extract_cluster_zone(self, details: Dict[str, object]) -> Optional[str]:
        head_node = details.get("headNode")
        if isinstance(head_node, dict):
            zone = head_node.get("availabilityZone") or head_node.get("AvailabilityZone")
            if isinstance(zone, str):
                return zone
        return None

    def _find_existing_cluster(self) -> Optional[str]:
        LOGGER.debug("Checking for existing clusters in %s", self.config.aws.region)
        cmd = ["pcluster", "list-clusters", "--region", self.config.aws.region]
        result = self._run_command(cmd, check=False, env=self._pcluster_env())
        if result.returncode != 0:
            LOGGER.debug(
                "Unable to list clusters (exit %s): %s",
                result.returncode,
                result.stderr.decode(errors="ignore"),
            )
            return None
        payload = self._load_pcluster_json(result.stdout)
        if not isinstance(payload, dict):
            LOGGER.debug("Unexpected list-clusters output: %s", result.stdout.decode(errors="ignore"))
            return None
        clusters = payload.get("clusters")
        if not isinstance(clusters, list):
            return None
        preferred_zone = self.config.cluster.preferred_availability_zone
        for cluster in clusters:
            if not isinstance(cluster, dict):
                continue
            name = cluster.get("clusterName")
            if not name:
                continue
            details = self._describe_cluster(name)
            if not details:
                continue
            if preferred_zone:
                cluster_zone = self._extract_cluster_zone(details)
                if cluster_zone and cluster_zone != preferred_zone:
                    LOGGER.debug(
                        "Skipping cluster %s due to availability zone mismatch (%s != %s)",
                        name,
                        cluster_zone,
                        preferred_zone,
                    )
                    continue
                if not cluster_zone:
                    LOGGER.debug(
                        "Cluster %s missing availability zone information; unable to enforce preference",
                        name,
                    )
            if self._cluster_is_ready(details):
                return name
        return None

    def _create_cluster(self, work_yaml: Dict[str, object]) -> str:
        cluster_name = work_yaml.get("cluster_name") or f"daylily-{int(time.time())}"
        LOGGER.info("Creating new ephemeral cluster %s", cluster_name)
        cmd = ["./bin/daylily-create-ephemeral-cluster", "--cluster-name", cluster_name]
        if self.config.cluster.template_path:
            cmd.extend(["--config", self.config.cluster.template_path])
        if self.config.cluster.repo_tag:
            cmd.extend(["--repo-tag", self.config.cluster.repo_tag])
        env = os.environ.copy()
        contact_email = self.config.cluster.contact_email or "you@email.com"
        env["DAY_CONTACT_EMAIL"] = contact_email
        env.pop("DAY_DISABLE_AUTO_SELECT", None)
        result = self._run_command(cmd, check=True, env=env)
        LOGGER.debug(
            "Cluster creation stdout: %s", result.stdout.decode(errors="ignore")
        )
        return cluster_name

    # ------------------------------------------------------------------
    # File and S3 helpers
    # ------------------------------------------------------------------
    
    def _assert_s3_uri_exists(self, uri: str) -> None:
        """Verify that s3://bucket/key exists."""
        if not uri.startswith("s3://"):
            raise MonitorError(f"Invalid S3 URI: {uri}")
        remainder = uri[5:]
        parts = remainder.split("/", 1)
        if len(parts) != 2 or not parts[0] or not parts[1]:
            raise MonitorError(f"Invalid S3 URI: {uri}")
        bucket, key = parts
        try:
            self._s3.head_object(Bucket=bucket, Key=key)
        except ClientError as exc:
            raise MonitorError(f"Referenced S3 object not found: {uri}") from exc

    def _assert_sample_file_exists(self, workset: Workset, relative_path: str) -> None:
        """Verify that a sample_data file exists inside the workset prefix."""
        key = f"{workset.prefix}{SAMPLE_DATA_DIRNAME}/{relative_path.lstrip('/')}"
        try:
            self._s3.head_object(Bucket=self.config.monitor.bucket, Key=key)
        except ClientError as exc:
                raise MonitorError(f"Sample data file missing for {workset.name}: {relative_path}") from exc
    def _read_required_object(self, workset_prefix: str, filename: str) -> bytes:
        bucket = self.config.monitor.bucket
        key = f"{workset_prefix}{filename}"
        try:
            response = self._s3.get_object(Bucket=bucket, Key=key)
        except ClientError as exc:
            raise MonitorError(
                f"Missing required file {filename} in {workset_prefix}: {exc}"
            ) from exc
        return response["Body"].read()

    def _load_work_yaml(self, workset: Workset) -> Optional[Dict[str, object]]:
        cached = self._work_yaml_cache.get(workset.name)
        if cached is not None:
            return cached
        try:
            data_bytes = self._read_required_object(workset.prefix, WORK_YAML_NAME)
        except MonitorError:
            return None
        try:
            data = yaml.safe_load(data_bytes.decode("utf-8"))
        except yaml.YAMLError as exc:
            LOGGER.warning("Unable to parse %s for %s: %s", WORK_YAML_NAME, workset.name, exc)
            return None
        if isinstance(data, dict):
            self._work_yaml_cache[workset.name] = data
            return data
        return None

    def _write_sentinel(self, workset: Workset, sentinel_name: str, value: str) -> None:
        bucket = self.config.monitor.bucket
        key = f"{workset.prefix}{sentinel_name}"
        body = value.encode("utf-8")
        LOGGER.debug("Writing sentinel %s for %s", sentinel_name, workset.name)
        if self.dry_run:
            return
        self._s3.put_object(Bucket=bucket, Key=key, Body=body)

    def _delete_sentinel(self, workset: Workset, sentinel_name: str) -> None:
        bucket = self.config.monitor.bucket
        key = f"{workset.prefix}{sentinel_name}"
        LOGGER.debug("Deleting sentinel %s for %s", sentinel_name, workset.name)
        if self.dry_run:
            return
        self._s3.delete_object(Bucket=bucket, Key=key)

    def _write_temp_file(self, workset: Workset, filename: str, data: bytes) -> Path:
        temp_dir = Path("/tmp") / f"daylily-workset-{workset.name}"
        temp_dir.mkdir(parents=True, exist_ok=True)
        path = temp_dir / filename
        path.write_bytes(data)
        return path

    def _local_stage_root(self) -> Optional[Path]:
        root = self.config.pipeline.local_stage_root
        if not root:
            return None
        path = Path(root).expanduser()
        path.mkdir(parents=True, exist_ok=True)
        return path

    def _copy_manifest_to_local(self, workset: Workset, manifest_path: Path) -> Path:
        """Optional mirror of stage_samples.tsv to a local cache (for audit/inspection)."""
        local_root = self._local_stage_root()
        if not local_root:
            return manifest_path
        destination_dir = local_root / workset.name
        destination_dir.mkdir(parents=True, exist_ok=True)
        destination_path = destination_dir / manifest_path.name
        shutil.copy2(manifest_path, destination_path)
        LOGGER.info("Copied stage manifest for %s to %s", workset.name, destination_path)
        return destination_path

    def _relative_manifest_argument(self, manifest_path: Path) -> str:
        """Prefer a relative path when possible, otherwise absolute."""
        try:
            return str(manifest_path.relative_to(Path.cwd()))
        except ValueError:
            return str(manifest_path)

    def _stage_reference_bucket(self) -> str:
        """
        Return the S3 reference bucket to pass to the staging command.
        Uses pipeline.reference_bucket if set; otherwise defaults to the monitor's bucket.
        Always ends with a trailing '/'.
        """
        bucket = self.config.pipeline.reference_bucket
        if not bucket:
            bucket = f"s3://{self.config.monitor.bucket}"
        if not bucket.endswith("/"):
            bucket += "/"
        return bucket

    def _read_object_text(self, bucket: str, key: str) -> str:
        response = self._s3.get_object(Bucket=bucket, Key=key)
        return response["Body"].read().decode("utf-8")

    def _should_process(self, workset: Workset) -> bool:
        if not self._process_directories:
            return True
        return workset.name in self._process_directories

    def _generate_report_rows(
        self, worksets: Sequence[Workset], *, include_details: bool
    ) -> List[WorksetReportRow]:
        rows = [
            self._summarize_workset(workset, include_details=include_details)
            for workset in worksets
            if self._should_process(workset)
        ]
        rows.sort(key=self._report_sort_key)
        return rows

    def _normalise_timestamp(
        self, value: Optional[str]
    ) -> Tuple[Optional[dt.datetime], Optional[str]]:
        if not value:
            return None, None
        text = value.strip()
        if not text:
            return None, None
        parsed = self._parse_timestamp(text)
        if parsed:
            display = self._format_timestamp(parsed)
            return parsed, display
        return None, self._trim_timestamp_text(text)

    def _trim_timestamp_text(self, text: str) -> str:
        if "." in text:
            prefix, _, suffix = text.partition(".")
            if "Z" in suffix:
                return prefix + "Z"
            return prefix
        return text

    def _parse_timestamp(self, text: Optional[str]) -> Optional[dt.datetime]:
        if not text:
            return None
        candidate = text.strip()
        if not candidate:
            return None
        if candidate.endswith("Z"):
            candidate = candidate[:-1] + "+00:00"
        try:
            return dt.datetime.fromisoformat(candidate)
        except ValueError:
            return None

    def _format_timestamp(self, value: dt.datetime) -> str:
        if value.tzinfo:
            value = value.astimezone(dt.timezone.utc)
        return value.replace(microsecond=0).isoformat().replace("+00:00", "Z")

    def _summarize_workset(
        self, workset: Workset, *, include_details: bool
    ) -> WorksetReportRow:
        sentinels = workset.sentinels

        def sentinel_parts(name: str) -> Tuple[Optional[str], Optional[str]]:
            raw = sentinels.get(name)
            if not raw:
                return None, None
            text = raw.strip()
            if not text:
                return None, None
            if "\t" in text:
                timestamp, detail = text.split("\t", 1)
            else:
                timestamp, detail = text, None
            return timestamp or None, detail or None

        sentinel_cache: Dict[str, Tuple[Optional[str], Optional[str]]] = {}

        def cached_sentinel_parts(name: str) -> Tuple[Optional[str], Optional[str]]:
            if name not in sentinel_cache:
                sentinel_cache[name] = sentinel_parts(name)
            return sentinel_cache[name]

        ignore_ts, ignore_detail = cached_sentinel_parts(SENTINEL_FILES["ignore"])
        complete_ts, _ = cached_sentinel_parts(SENTINEL_FILES["complete"])
        error_ts, error_detail = cached_sentinel_parts(SENTINEL_FILES["error"])
        in_progress_ts, _ = cached_sentinel_parts(SENTINEL_FILES["in_progress"])
        lock_ts, _ = cached_sentinel_parts(SENTINEL_FILES["lock"])
        ready_ts, _ = cached_sentinel_parts(SENTINEL_FILES["ready"])

        start_source = (
            in_progress_ts
            or lock_ts
            or ready_ts
            or complete_ts
            or error_ts
            or ignore_ts
        )
        start_dt, start_display = self._normalise_timestamp(start_source)

        if ignore_ts:
            other_state: Optional[str] = None
            for sentinel_name, state_name in [
                ("error", "error"),
                ("complete", "complete"),
                ("in_progress", "in-progress"),
                ("lock", "locked"),
                ("ready", "ready"),
            ]:
                other_ts, _ = cached_sentinel_parts(SENTINEL_FILES[sentinel_name])
                if other_ts:
                    other_state = state_name
                    break
            display_state = "ignored"
            if other_state:
                display_state = f"ignored ({other_state})"
            row = WorksetReportRow(
                workset.name,
                "ignored",
                start_dt,
                start_display,
                ignore_detail,
                workset.has_required_files,
                display_state=display_state,
            )
            if include_details:
                row.metrics = self._collect_report_metrics(
                    workset,
                    state="ignored",
                    start_dt=start_dt,
                    end_ts=ignore_ts,
                    include_details=True,
                )
            return row

        if error_ts:
            row = WorksetReportRow(
                workset.name,
                "error",
                start_dt,
                start_display,
                error_detail,
                workset.has_required_files,
            )
            if include_details:
                row.metrics = self._collect_report_metrics(
                    workset,
                    state="error",
                    start_dt=start_dt,
                    end_ts=error_ts,
                    include_details=True,
                )
            return row

        if complete_ts:
            row = WorksetReportRow(
                workset.name,
                "complete",
                start_dt,
                start_display,
                None,
                workset.has_required_files,
            )
            if include_details:
                row.metrics = self._collect_report_metrics(
                    workset,
                    state="complete",
                    start_dt=start_dt,
                    end_ts=complete_ts,
                    include_details=True,
                )
            return row

        if in_progress_ts:
            row = WorksetReportRow(
                workset.name,
                "in-progress",
                start_dt,
                start_display,
                None,
                workset.has_required_files,
            )
            if include_details:
                row.metrics = self._collect_report_metrics(
                    workset,
                    state="in-progress",
                    start_dt=start_dt,
                    end_ts=None,
                    include_details=False,
                )
            return row

        if lock_ts:
            start_dt, start_display = self._normalise_timestamp(lock_ts)
            return WorksetReportRow(
                workset.name,
                "locked",
                start_dt,
                start_display,
                None,
                workset.has_required_files,
            )

        if ready_ts:
            start_dt, start_display = self._normalise_timestamp(ready_ts)
            return WorksetReportRow(
                workset.name,
                "ready",
                start_dt,
                start_display,
                None,
                workset.has_required_files,
            )

        return WorksetReportRow(
            workset.name,
            "unknown",
            None,
            None,
            None,
            workset.has_required_files,
        )

    def _collect_report_metrics(
        self,
        workset: Workset,
        *,
        state: str,
        start_dt: Optional[dt.datetime],
        end_ts: Optional[str],
        include_details: bool,
    ) -> WorksetReportMetrics:
        terminal_states = {"complete", "error", "ignored"}
        end_dt, _ = self._normalise_timestamp(end_ts)
        cached: Optional[WorksetReportMetrics] = None
        if state in terminal_states:
            cached = self._load_report_cache(workset)
        metrics = dataclasses.replace(cached) if cached else WorksetReportMetrics()
        if end_dt:
            metrics.end_dt = end_dt

        work_yaml = self._load_work_yaml(workset) or {}
        budget_value = work_yaml.get("budget")
        if isinstance(budget_value, str):
            budget_value = budget_value.strip() or None
        elif budget_value is not None:
            budget_value = str(budget_value)
        if budget_value:
            metrics.budget_name = budget_value

        clone_args_raw = self._yaml_get_str(
            work_yaml,
            ["day_clone_args", "day-clone", "clone_args", "clone-args", "clone"],
        ) or ""
        clone_args = self._format_clone_args(clone_args_raw, workset, work_yaml)
        clone_args = self._apply_budget_argument(clone_args, work_yaml)
        clone_command_template = self.config.pipeline.clone_command
        metrics.clone_command = clone_command_template.format(clone_args=clone_args).strip()

        run_suffix = self._yaml_get_str(
            work_yaml,
            ["dy_r", "dy-r", "dy", "run", "run_suffix", "run-suffix", "run_cmd", "run-command"],
        )
        if run_suffix:
            metrics.pipeline_command = (self.config.pipeline.run_prefix or "") + run_suffix

        cluster_name = self._load_workset_cluster(workset) or metrics.cluster_name
        if not cluster_name:
            fallback_cluster = work_yaml.get("cluster_name") or self.config.cluster.reuse_cluster_name
            if isinstance(fallback_cluster, str) and fallback_cluster.strip():
                cluster_name = fallback_cluster.strip()
        if cluster_name:
            metrics.cluster_name = cluster_name

        if metrics.region_az is None and include_details and metrics.cluster_name:
            details = self._describe_cluster(metrics.cluster_name)
            if details:
                zone = self._extract_cluster_zone(details)
                if zone:
                    metrics.region_az = zone

        start_seconds = None
        start_aware: Optional[dt.datetime] = None
        if start_dt:
            start_aware = start_dt
            if start_aware.tzinfo is None:
                start_aware = start_aware.replace(tzinfo=dt.timezone.utc)
        if start_aware:
            if metrics.end_dt:
                end_aware = metrics.end_dt
                if end_aware.tzinfo is None:
                    end_aware = end_aware.replace(tzinfo=dt.timezone.utc)
                start_seconds = max((end_aware - start_aware).total_seconds(), 0.0)
            else:
                now = dt.datetime.utcnow().replace(tzinfo=dt.timezone.utc)
                start_seconds = max((now - start_aware).total_seconds(), 0.0)
        if start_seconds is not None:
            metrics.run_seconds = start_seconds

        if include_details and metrics.cluster_name:
            pipeline_dir = self._load_pipeline_location(workset)
            if pipeline_dir is not None:
                metrics_cmd = (
                    "cd "
                    + shlex.quote(str(pipeline_dir))
                    + " && ./bin/daylily-workset-metrics --json ."
                )
                result = self._run_headnode_command(
                    metrics.cluster_name, metrics_cmd, check=False, shell=True
                )
                if result.returncode != 0:
                    LOGGER.debug(
                        "Metrics command for %s exited %s: %s",
                        workset.name,
                        result.returncode,
                        result.stderr.decode("utf-8", errors="ignore"),
                    )
                    script = workset_metrics.remote_metrics_script()
                    fallback_cmd = (
                        f"cd {shlex.quote(str(pipeline_dir))} && python3 - <<'PY'\n{script}\nPY"
                    )
                    result = self._run_headnode_command(
                        metrics.cluster_name, fallback_cmd, check=False, shell=True
                    )
                if result.returncode == 0:
                    payload = result.stdout.decode("utf-8", errors="ignore").strip()
                    try:
                        data = json.loads(payload)
                    except json.JSONDecodeError:
                        LOGGER.debug(
                            "Unable to decode metrics output for %s: %s",
                            workset.name,
                            payload,
                        )
                    else:
                        metrics.samples = int(data.get("samples", metrics.samples or 0))
                        metrics.sample_libraries = int(
                            data.get("sample_libraries", metrics.sample_libraries or 0)
                        )
                        metrics.fastq_files = int(
                            data.get("fastq_files", metrics.fastq_files or 0)
                        )
                        metrics.fastq_bytes = int(
                            data.get("fastq_bytes", metrics.fastq_bytes or 0)
                        )
                        metrics.cram_files = int(
                            data.get("cram_files", metrics.cram_files or 0)
                        )
                        metrics.cram_bytes = int(
                            data.get("cram_bytes", metrics.cram_bytes or 0)
                        )
                        metrics.vcf_files = int(
                            data.get("vcf_files", metrics.vcf_files or 0)
                        )
                        metrics.vcf_bytes = int(
                            data.get("vcf_bytes", metrics.vcf_bytes or 0)
                        )
                        metrics.results_bytes = int(
                            data.get("results_bytes", metrics.results_bytes or 0)
                        )
                        metrics.ec2_cost = float(
                            data.get("ec2_cost", metrics.ec2_cost or 0.0)
                        )
                elif result.returncode != 0:
                    LOGGER.debug(
                        "Metrics command for %s exited %s: %s",
                        workset.name,
                        result.returncode,
                        result.stderr.decode("utf-8", errors="ignore"),
                    )

        if metrics.results_bytes:
            metrics.s3_daily_cost = workset_metrics.storage_daily_cost(metrics.results_bytes)
        if metrics.cram_bytes:
            region_cost, internet_cost = workset_metrics.transfer_costs(metrics.cram_bytes)
            metrics.cram_region_cost = region_cost
            metrics.cram_internet_cost = internet_cost
        if metrics.vcf_bytes:
            region_cost, internet_cost = workset_metrics.transfer_costs(metrics.vcf_bytes)
            metrics.vcf_region_cost = region_cost
            metrics.vcf_internet_cost = internet_cost

        if state in terminal_states and include_details:
            self._save_report_cache(workset, metrics)

        return metrics

    def _report_sort_key(self, row: WorksetReportRow) -> Tuple[int, str]:
        priority = STATE_PRIORITIES.get(row.state, max(STATE_PRIORITIES.values()) + 1)
        return priority, row.name

    def _wrap_cell_lines(self, value: object) -> List[str]:
        text = "" if value is None else str(value)
        if text == "":
            return [""]
        width = max(self._wrap_char, 1)
        lines: List[str] = []
        for segment in text.splitlines() or [""]:
            if not segment:
                lines.append("")
                continue
            if width <= 0:
                lines.append(segment)
                continue
            wrapped = textwrap.wrap(
                segment,
                width=width,
                break_long_words=True,
                break_on_hyphens=False,
                replace_whitespace=False,
                drop_whitespace=False,
            )
            if not wrapped:
                lines.append("")
            else:
                lines.extend(wrapped)
        return lines or [""]

    def _format_state_cell(self, text: str, row: WorksetReportRow) -> str:
        state_color = STATE_COLORS.get(row.state, STATE_COLORS["unknown"])
        reset = "\033[0m"
        return f"{state_color}{text}{reset}"

    def _format_valid_cell(self, text: str, row: WorksetReportRow) -> str:
        if row.has_required_files:
            return text
        return f"\033[31m{text}\033[0m"

    def _render_term_report(
        self, rows: Sequence[WorksetReportRow], *, minimal: bool
    ) -> None:
        if not rows:
            print("No worksets matched the selection.")
            return

        if minimal:
            columns = [
                ReportColumn("Workset", lambda r: r.name),
                ReportColumn("State", lambda r: r.state_text, formatter=self._format_state_cell),
                ReportColumn("StartDT", lambda r: r.start_display or "-"),
                ReportColumn(
                    "Valid",
                    lambda r: "yes" if r.has_required_files else "NO",
                    formatter=self._format_valid_cell,
                    min_width=8,
                ),
                ReportColumn("Detail", lambda r: r.detail or ""),
            ]
        else:
            columns = [
                ReportColumn("Workset", lambda r: r.name),
                ReportColumn("State", lambda r: r.state_text, formatter=self._format_state_cell),
                ReportColumn("StartDT", lambda r: r.start_display or "-"),
                ReportColumn(
                    "EndDT",
                    lambda r: self._format_timestamp(r.metrics.end_dt)
                    if r.metrics and r.metrics.end_dt
                    else "-",
                ),
                ReportColumn(
                    "RunTime",
                    lambda r: self._format_runtime(r.metrics.run_seconds)
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "Valid",
                    lambda r: "yes" if r.has_required_files else "NO",
                    formatter=self._format_valid_cell,
                ),
                ReportColumn(
                    "Cluster",
                    lambda r: r.metrics.cluster_name
                    if r.metrics and r.metrics.cluster_name
                    else "-",
                ),
                ReportColumn(
                    "RegionAZ",
                    lambda r: r.metrics.region_az
                    if r.metrics and r.metrics.region_az
                    else "-",
                ),
                ReportColumn(
                    "Budget",
                    lambda r: r.metrics.budget_name
                    if r.metrics and r.metrics.budget_name
                    else "-",
                ),
                ReportColumn(
                    "PipelineCmd",
                    lambda r: r.metrics.pipeline_command
                    if r.metrics and r.metrics.pipeline_command
                    else "-",
                ),
                ReportColumn(
                    "CloneCmd",
                    lambda r: r.metrics.clone_command
                    if r.metrics and r.metrics.clone_command
                    else "-",
                ),
                ReportColumn(
                    "Samps",
                    lambda r: self._format_integer(r.metrics.samples)
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "SampLibs",
                    lambda r: self._format_integer(r.metrics.sample_libraries)
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "Fastqs",
                    lambda r: self._format_count_size(
                        r.metrics.fastq_files, r.metrics.fastq_bytes
                    )
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "CRAMs",
                    lambda r: self._format_count_size(
                        r.metrics.cram_files, r.metrics.cram_bytes
                    )
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "VCFs",
                    lambda r: self._format_count_size(
                        r.metrics.vcf_files, r.metrics.vcf_bytes
                    )
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "Results",
                    lambda r: self._format_size(r.metrics.results_bytes)
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "S3DailyCost",
                    lambda r: self._format_currency_value(r.metrics.s3_daily_cost)
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "CRAM Xfer",
                    lambda r: self._format_transfer(
                        r.metrics.cram_region_cost, r.metrics.cram_internet_cost
                    )
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "VCF Xfer",
                    lambda r: self._format_transfer(
                        r.metrics.vcf_region_cost, r.metrics.vcf_internet_cost
                    )
                    if r.metrics
                    else "-",
                ),
                ReportColumn(
                    "EC2Cost",
                    lambda r: self._format_currency_value(r.metrics.ec2_cost)
                    if r.metrics
                    else "-",
                ),
                ReportColumn("Detail", lambda r: r.detail or ""),
            ]

        wrapped_rows: List[List[List[str]]] = []
        for row in rows:
            row_lines: List[List[str]] = []
            for column in columns:
                row_lines.append(self._wrap_cell_lines(column.value_func(row)))
            wrapped_rows.append(row_lines)

        widths: List[int] = []
        for index, column in enumerate(columns):
            max_len = len(column.title)
            for row_lines in wrapped_rows:
                for line in row_lines[index]:
                    max_len = max(max_len, len(line))
            if column.min_width:
                max_len = max(max_len, column.min_width)
            widths.append(max_len)

        header_cells = [
            f"{column.title:<{widths[idx]}}" for idx, column in enumerate(columns)
        ]
        header = "  ".join(header_cells)
        print(header)
        print("-" * len(header))

        for row, row_lines in zip(rows, wrapped_rows):
            max_lines = max(len(lines) for lines in row_lines)
            for line_index in range(max_lines):
                parts: List[str] = []
                for col_index, column in enumerate(columns):
                    lines = row_lines[col_index]
                    line_text = lines[line_index] if line_index < len(lines) else ""
                    padded = f"{line_text:<{widths[col_index]}}"
                    if column.formatter:
                        padded = column.formatter(padded, row)
                    parts.append(padded)
                print("  ".join(parts))

    def _write_delimited_report(
        self, rows: Sequence[WorksetReportRow], path: Path, delimiter: str, *, minimal: bool
    ) -> None:
        with path.open("w", encoding="utf-8", newline="") as handle:
            writer = csv.writer(handle, delimiter=delimiter)
            self._write_rows(writer, rows, minimal)

    def _write_rows(
        self, writer: csv.writer, rows: Sequence[WorksetReportRow], minimal: bool
    ) -> None:
        if minimal:
            writer.writerow(["workset", "state", "start_dt", "detail", "valid"])
            for row in rows:
                writer.writerow(
                    [
                        row.name,
                        row.state_text,
                        row.start_display or "",
                        row.detail or "",
                        "yes" if row.has_required_files else "no",
                    ]
                )
            return

        writer.writerow(
            [
                "workset",
                "state",
                "start_dt",
                "end_dt",
                "run_seconds",
                "detail",
                "valid",
                "cluster_name",
                "region_az",
                "budget_name",
                "pipeline_command",
                "clone_command",
                "samples",
                "sample_libraries",
                "fastq_files",
                "fastq_bytes",
                "cram_files",
                "cram_bytes",
                "vcf_files",
                "vcf_bytes",
                "results_bytes",
                "s3_daily_cost",
                "cram_region_cost",
                "cram_internet_cost",
                "vcf_region_cost",
                "vcf_internet_cost",
                "ec2_cost",
            ]
        )
        for row in rows:
            metrics = row.metrics or WorksetReportMetrics()
            writer.writerow(
                [
                    row.name,
                    row.state_text,
                    row.start_display or "",
                    self._format_timestamp(metrics.end_dt) if metrics.end_dt else "",
                    f"{metrics.run_seconds:.0f}" if metrics.run_seconds is not None else "",
                    row.detail or "",
                    "yes" if row.has_required_files else "no",
                    metrics.cluster_name or "",
                    metrics.region_az or "",
                    metrics.budget_name or "",
                    metrics.pipeline_command or "",
                    metrics.clone_command or "",
                    metrics.samples if metrics.samples is not None else "",
                    metrics.sample_libraries if metrics.sample_libraries is not None else "",
                    metrics.fastq_files if metrics.fastq_files is not None else "",
                    metrics.fastq_bytes if metrics.fastq_bytes is not None else "",
                    metrics.cram_files if metrics.cram_files is not None else "",
                    metrics.cram_bytes if metrics.cram_bytes is not None else "",
                    metrics.vcf_files if metrics.vcf_files is not None else "",
                    metrics.vcf_bytes if metrics.vcf_bytes is not None else "",
                    metrics.results_bytes if metrics.results_bytes is not None else "",
                    metrics.s3_daily_cost if metrics.s3_daily_cost is not None else "",
                    metrics.cram_region_cost if metrics.cram_region_cost is not None else "",
                    metrics.cram_internet_cost if metrics.cram_internet_cost is not None else "",
                    metrics.vcf_region_cost if metrics.vcf_region_cost is not None else "",
                    metrics.vcf_internet_cost if metrics.vcf_internet_cost is not None else "",
                    metrics.ec2_cost if metrics.ec2_cost is not None else "",
                ]
            )

    def _format_runtime(self, seconds: Optional[float]) -> str:
        if seconds is None:
            return "-"
        duration = dt.timedelta(seconds=int(seconds))
        text = str(duration)
        if duration < dt.timedelta(days=1):
            # Ensure HH:MM:SS format
            total_seconds = int(seconds)
            hours, remainder = divmod(total_seconds, 3600)
            minutes, seconds = divmod(remainder, 60)
            text = f"{hours:02d}:{minutes:02d}:{int(seconds):02d}"
        return text

    def _format_integer(self, value: Optional[int]) -> str:
        if value is None:
            return "-"
        return str(value)

    def _format_size(self, value: Optional[int]) -> str:
        if not value:
            return "-"
        return workset_metrics.format_gib(value)

    def _format_currency_value(self, value: Optional[float]) -> str:
        if value is None:
            return "-"
        return workset_metrics.format_currency(value)

    def _format_count_size(
        self, count: Optional[int], size_bytes: Optional[int]
    ) -> str:
        if count is None and size_bytes is None:
            return "-"
        count_text = self._format_integer(count) if count is not None else "0"
        size_text = self._format_size(size_bytes)
        if size_text == "-":
            size_text = "0.00 GB"
        return f"{count_text} ({size_text})"

    def _format_transfer(
        self, region_cost: Optional[float], internet_cost: Optional[float]
    ) -> str:
        if region_cost is None and internet_cost is None:
            return "-"
        region = self._format_currency_value(region_cost)
        internet = self._format_currency_value(internet_cost)
        return f"region {region} / internet {internet}"

    def report(self, target: str, *, minimal: bool) -> None:
        worksets = list(self._discover_worksets())
        rows = self._generate_report_rows(worksets, include_details=not minimal)
        if self._process_directories:
            found = {row.name for row in rows}
            missing = sorted(self._process_directories - found)
            for name in missing:
                LOGGER.warning("Requested workset %s was not found", name)

        if target.lower() == "term":
            self._render_term_report(rows, minimal=minimal)
            return

        # Buffer CSV/TSV once so we can write to S3 or local
        def _serialize(rows: Sequence[WorksetReportRow], delim: str) -> bytes:
            from io import StringIO

            buf = StringIO()
            writer = csv.writer(buf, delimiter=delim)
            self._write_rows(writer, rows, minimal)
            return buf.getvalue().encode("utf-8")

        if target.startswith("s3://"):
            delim = "\t" if target.lower().endswith(".tsv") else ","
            body = _serialize(rows, delim)
            bucket, key = target[5:].split("/", 1)
            if self.dry_run:
                print(f"[DRY-RUN] put_object s3://{bucket}/{key} ({len(body)} bytes)")
            else:
                self._s3.put_object(Bucket=bucket, Key=key, Body=body)
            LOGGER.info("Wrote report with %d entries to %s", len(rows), target)
            return

        # local file
        path = Path(target)
        suffix = path.suffix.lower()
        delim = "\t" if suffix == ".tsv" else ","
        if path.parent and not path.parent.exists():
            path.parent.mkdir(parents=True, exist_ok=True)
        self._write_delimited_report(rows, path, delim, minimal=minimal)
        LOGGER.info("Wrote report with %d entries to %s", len(rows), path)

    # ------------------------------------------------------------------
    # Command helpers
    # ------------------------------------------------------------------
    def _run_monitored_command(
        self,
        command_label: str,
        command: Sequence[str] | str,
        *,
        check: bool,
        cwd: Optional[Path] = None,
        shell: bool = False,
        env: Optional[Dict[str, str]] = None,
    ) -> subprocess.CompletedProcess:
        if isinstance(command, (str, bytes)) and not shell:
            cmd_display = command
        else:
            cmd_display = " ".join(command) if not isinstance(command, str) else command
        try:
            return self._run_command(
                command, check=check, cwd=cwd, shell=shell, env=env
            )
        except MonitorError as exc:
            raise CommandFailedError(command_label, str(cmd_display)) from exc

    def _run_command(
        self,
        command: Sequence[str] | str,
        *,
        check: bool,
        cwd: Optional[Path] = None,
        shell: bool = False,
        env: Optional[Dict[str, str]] = None,
    ) -> subprocess.CompletedProcess:
        if isinstance(command, (str, bytes)) and not shell:
            cmd_display = command
        else:
            cmd_display = " ".join(command) if not isinstance(command, str) else command
        LOGGER.debug("Executing command: %s", cmd_display)

        action = "DRY-RUN" if self.dry_run else "EXEC"
        if self.dry_run or self.debug:
            print(f"[{action}] {cmd_display}")

        if self.dry_run:
            return subprocess.CompletedProcess(args=command, returncode=0, stdout=b"", stderr=b"")

        run_command = command
        run_shell = shell
        if isinstance(command, str) and not shell:
            run_command = shlex.split(command)
        result = subprocess.run(
            run_command,
            check=False,
            cwd=cwd,
            shell=run_shell,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            env=env,
        )
        if check and result.returncode != 0:
            LOGGER.error(
                "Command failed (%s): %s",
                result.returncode,
                result.stderr.decode(errors="ignore"),
            )
            raise MonitorError(f"Command failed: {cmd_display}")
        return result

    def _ssh_identity(self) -> Optional[str]:
        identity = self.config.pipeline.ssh_identity_file
        if not identity:
            return None
        return str(Path(identity).expanduser())

    def _ssh_user(self) -> str:
        return self.config.pipeline.ssh_user or "ubuntu"

    def _ssh_options(self) -> List[str]:
        options = ["-o", "StrictHostKeyChecking=no", "-o", "UserKnownHostsFile=/dev/null"]
        if self.config.pipeline.ssh_extra_args:
            options.extend(self.config.pipeline.ssh_extra_args)
        return options

    def _headnode_ip(self, cluster_name: str) -> str:
        cached = self._headnode_ips.get(cluster_name)
        if cached:
            return cached
        cmd = ["pcluster", "describe-cluster-instances", "--region", self.config.aws.region, "-n", cluster_name]
        result = self._run_command(cmd, check=True, env=self._pcluster_env())
        payload = self._load_pcluster_json(result.stdout)
        if isinstance(payload, dict):
            instances = payload.get("instances")
            if isinstance(instances, list):
                for instance in instances:
                    if not isinstance(instance, dict):
                        continue
                    node_type = instance.get("nodeType") or instance.get("NodeType")
                    if isinstance(node_type, str) and node_type.lower() == "headnode":
                        ip = instance.get("publicIpAddress") or instance.get("PublicIpAddress")
                        if isinstance(ip, str) and ip:
                            self._headnode_ips[cluster_name] = ip
                            return ip
        raise MonitorError(f"Unable to determine head node address for {cluster_name}")

    def _build_remote_command(
        self,
        command: Sequence[str] | str,
        *,
        cwd: Optional[str] = None,
        shell: bool = False,
    ) -> str:
        if isinstance(command, (list, tuple)):
            command_str = " ".join(shlex.quote(str(part)) for part in command)
        else:
            command_str = command if shell else " ".join(shlex.quote(part) for part in shlex.split(command))
        if cwd:
            command_str = f"cd {shlex.quote(cwd)} && {command_str}"
        return f"bash -lc {shlex.quote(command_str)}"


    def _build_ssh_command(
        self,
        cluster_name: str,
        command: Sequence[str] | str,
        *,
        cwd: Optional[str] = None,
        shell: bool = False,
    ) -> List[str]:
        remote_command = self._build_remote_command(command, cwd=cwd, shell=shell)
        headnode = self._headnode_ip(cluster_name)
        ssh_cmd: List[str] = ["ssh"]
        identity = self._ssh_identity()
        if identity:
            ssh_cmd.extend(["-i", identity])
        ssh_cmd.extend(self._ssh_options())
        ssh_cmd.append(f"{self._ssh_user()}@{headnode}")
        ssh_cmd.append(remote_command)
        return ssh_cmd

    def _build_scp_command(
        self,
        cluster_name: str,
        local_path: Path,
        remote_path: PurePosixPath,
    ) -> List[str]:
        headnode = self._headnode_ip(cluster_name)
        scp_cmd: List[str] = ["scp"]
        identity = self._ssh_identity()
        if identity:
            scp_cmd.extend(["-i", identity])
        scp_cmd.extend(self._ssh_options())
        remote_target = f"{self._ssh_user()}@{headnode}:{remote_path}"
        scp_cmd.extend([str(local_path), remote_target])
        return scp_cmd

    def _run_headnode_command(
        self,
        cluster_name: str,
        command: Sequence[str] | str,
        *,
        check: bool,
        cwd: Optional[str] = None,
        shell: bool = False,
    ) -> subprocess.CompletedProcess:
        ssh_cmd = self._build_ssh_command(cluster_name, command, cwd=cwd, shell=shell)
        return self._run_command(ssh_cmd, check=check)

    def _run_headnode_monitored_command(
        self,
        command_label: str,
        command: Sequence[str] | str,
        *,
        cluster_name: str,
        check: bool,
        cwd: Optional[str] = None,
        shell: bool = False,
    ) -> subprocess.CompletedProcess:
        ssh_cmd = self._build_ssh_command(cluster_name, command, cwd=cwd, shell=shell)
        return self._run_monitored_command(command_label, ssh_cmd, check=check)

    def _yaml_get_str(self, data: Dict[str, object], keys: Sequence[str]) -> Optional[str]:
        """Return first non-empty string for any of the given keys. If list, join; if dict, use suffix/args/cmd/command."""
        for k in keys:
            if k not in data:
                continue
            v = data[k]
            if v is None:
                continue
            if isinstance(v, str) and v.strip():
                return v.strip()
            if isinstance(v, (list, tuple)):
                parts = [str(x).strip() for x in v if str(x).strip()]
                if parts:
                    return " ".join(parts)
            if isinstance(v, dict):
                for candidate in ("suffix", "args", "cmd", "command"):
                    val = v.get(candidate)
                    if isinstance(val, str) and val.strip():
                        return val.strip()
        return None

    # ------------------------------------------------------------------
    # Session helpers
    # ------------------------------------------------------------------
    def _refresh_session(self) -> None:
        LOGGER.debug(
            "Validating STS caller identity for profile %s", self.config.aws.profile
        )
        try:
            identity = self._sts.get_caller_identity()
            LOGGER.info("Assumed identity: %s", identity.get("Arn"))
        except ClientError as exc:
            LOGGER.warning("Unable to validate AWS credentials: %s", exc)


def configure_logging(verbose: bool) -> None:
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(level=level, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")


def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Monitor S3 workset directories and launch Daylily pipelines"
    )
    parser.add_argument(
        "config",
        nargs="?",
        default=Path("daylily-workset-monitor.yaml"),
        type=Path,
        help="Path to the YAML configuration file",
    )
    parser.add_argument("--once", action="store_true", help="Run a single poll iteration and exit")
    parser.add_argument("--dry-run", action="store_true", help="Do not mutate S3 or execute commands")
    parser.add_argument(
        "--attempt-restart",
        action="store_true",
        help="Retry failed workset commands once starting from the failed command",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable debug logging")
    parser.add_argument("--debug", action="store_true", help="Print all commands executed (and in dry-run)")
    parser.add_argument(
        "--process-directory",
        dest="process_directories",
        metavar="NAME",
        nargs="+",
        help="Only process the specified workset directory names",
    )
    parser.add_argument(
        "--report",
        nargs="?",
        const="term",
        help=(
            "Generate a report of the monitored worksets. Use 'term' for a colorized table or provide "
            "a .tsv/.csv filename (local or s3://...)"
        ),
    )
    parser.add_argument(
        "--min-deets",
        action="store_true",
        help="When reporting, limit the output to the legacy column set",
    )
    parser.add_argument(
        "--wrap-char",
        type=int,
        default=100,
        help="When reporting to term, wrap column contents at this width",
    )
    args = parser.parse_args(argv)
    if args.wrap_char < 1:
        parser.error("--wrap-char must be a positive integer")
    if args.report and args.report.lower() != "term":
        target = args.report
        if target.startswith("s3://"):
            if not (target.lower().endswith(".csv") or target.lower().endswith(".tsv")):
                parser.error("--report s3://... must end with .tsv or .csv")
        else:
            suffix = Path(target).suffix.lower()
            if suffix not in {".tsv", ".csv"}:
                parser.error("--report expects 'term' or a path ending with .tsv or .csv")
    if args.min_deets and not args.report:
        parser.error("--min-deets requires --report")
    return args


def main(argv: Optional[Sequence[str]] = None) -> int:
    args = parse_args(argv)
    configure_logging(args.verbose)
    config = MonitorConfig.load(args.config)
    if args.once:
        config.monitor.continuous = False
    monitor = WorksetMonitor(
        config,
        dry_run=args.dry_run,
        debug=args.debug,
        process_directories=args.process_directories,
        attempt_restart=args.attempt_restart,
        wrap_char=args.wrap_char,
    )
    if args.report:
        monitor.report(args.report, minimal=args.min_deets)
        return 0

    monitor.run()
    return 0


if __name__ == "__main__":
    sys.exit(main())
