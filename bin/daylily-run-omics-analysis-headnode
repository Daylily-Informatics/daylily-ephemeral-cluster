#!/usr/bin/env python3
"""Launch daylily-omics-analysis inside a tmux session on the head node."""

from __future__ import annotations

import argparse
import json
import os
import shlex
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

# Ensure DAY-EC conda environment is active
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "helpers"))
from ensure_dayec import ensure_dayec
ensure_dayec(quiet=True)


SSH_OPTIONS: Tuple[str, ...] = (
    "-o",
    "StrictHostKeyChecking=no",
    "-o",
    "UserKnownHostsFile=/dev/null",
)


class CommandError(RuntimeError):
    """Raised when an external command fails."""


@dataclass
class RemoteConfig:
    stage_dir: str
    samples_path: str
    units_path: str


def normalize_remote_path(path: str) -> str:
    if path.startswith("~/"):
        return path.replace("~/", "/home/ubuntu/", 1)
    if path == "~":
        return "/home/ubuntu"
    return path


def run_command(
    command: Iterable[str],
    *,
    capture_output: bool = False,
    env: Optional[dict] = None,
    check: bool = True,
) -> subprocess.CompletedProcess:
    try:
        result = subprocess.run(  # type: ignore[call-arg]
            list(command),
            check=check,
            capture_output=capture_output,
            text=True,
            env=env,
        )
    except subprocess.CalledProcessError as exc:  # pragma: no cover
        stdout = exc.stdout or ""
        stderr = exc.stderr or ""
        message = f"Command failed ({exc.returncode}): {' '.join(command)}"
        if stdout:
            message += f"\nSTDOUT:\n{stdout.strip()}"
        if stderr:
            message += f"\nSTDERR:\n{stderr.strip()}"
        raise CommandError(message) from exc
    return result


def choose_from(prompt: str, options: List[str]) -> str:
    if not options:
        raise CommandError(f"No options available for: {prompt}")
    if len(options) == 1:
        return options[0]
    print(prompt)
    for idx, value in enumerate(options, start=1):
        print(f"  {idx}) {value}")
    while True:
        try:
            selection = int(input("Select an option: "))
        except ValueError:
            print("Please enter a number.")
            continue
        if 1 <= selection <= len(options):
            return options[selection - 1]
        print("Selection out of range; try again.")


def resolve_region(profile: str) -> str:
    env = {**os.environ, "AWS_PROFILE": profile} if profile else None
    result = run_command(
        ["aws", "ec2", "describe-regions", "--output", "json"],
        capture_output=True,
        env=env,
    )
    data = json.loads(result.stdout)
    regions = sorted(entry["RegionName"] for entry in data.get("Regions", []))
    if not regions:
        raise CommandError("Unable to retrieve AWS regions. Check AWS credentials.")
    default_region = os.environ.get("AWS_REGION") or os.environ.get("AWS_DEFAULT_REGION")
    if default_region and default_region in regions:
        return default_region
    return choose_from("Select AWS region:", regions)


def resolve_cluster(profile: str, region: str) -> str:
    env = {**os.environ, "AWS_PROFILE": profile} if profile else None
    result = run_command(
        ["pcluster", "list-clusters", "--region", region, "--output", "json"],
        capture_output=True,
        env=env,
    )
    data = json.loads(result.stdout or "{}")
    clusters = [entry.get("clusterName") for entry in data.get("clusters", [])]
    clusters = [name for name in clusters if name]
    if not clusters:
        raise CommandError(f"No ParallelCluster clusters found in region {region!r}.")
    return choose_from("Select cluster:", sorted(clusters))


def resolve_pem_file(path: Optional[str]) -> str:
    if path:
        expanded = os.path.expanduser(path)
        if not os.path.exists(expanded):
            raise CommandError(f"PEM file not found: {expanded}")
        return expanded
    pem_candidates = sorted(Path.home().glob(".ssh/*.pem"))
    if not pem_candidates:
        raise CommandError("No PEM files found under ~/.ssh. Provide one with --pem.")
    selection = choose_from("Select SSH PEM key:", [str(p) for p in pem_candidates])
    return selection


def fetch_headnode_ip(profile: str, region: str, cluster_name: str) -> str:
    env = {**os.environ, "AWS_PROFILE": profile} if profile else None
    cmd = [
        "pcluster",
        "describe-cluster",
        "--region",
        region,
        "--cluster-name",
        cluster_name,
        "--output",
        "json",
    ]
    try:
        result = run_command(cmd, capture_output=True, env=env)
        payload = json.loads(result.stdout)
        head_node = payload.get("cluster", {}).get("headNode") or payload.get("headNode", {})
        ip_address = head_node.get("publicIpAddress") or head_node.get("publicIp")
        if not ip_address:
            raise KeyError("publicIpAddress")
        return ip_address
    except (CommandError, json.JSONDecodeError, KeyError):  # pragma: no cover
        result = run_command(
            [
                "pcluster",
                "describe-cluster",
                "--region",
                region,
                "--cluster-name",
                cluster_name,
            ],
            capture_output=True,
            env=env,
        )
        for line in (result.stdout or "").splitlines():
            if "publicIpAddress" in line or "publicIp" in line:
                parts = line.replace("\"", "").replace(",", "").split(":", 1)
                if len(parts) == 2 and parts[1].strip():
                    return parts[1].strip()
        raise CommandError("Unable to determine head node IP address.")


def run_remote_script(pem: str, host: str, script: str) -> subprocess.CompletedProcess:
    remote_cmd = f"bash -lc {shlex.quote(script)}"
    return run_command(
        ["ssh", "-i", pem, *SSH_OPTIONS, f"ubuntu@{host}", remote_cmd],
        capture_output=True,
    )


def parse_remote_config(stdout: str) -> RemoteConfig:
    stage_dir = samples_path = units_path = None
    for line in stdout.splitlines():
        if line.startswith("__DAYLILY_STAGE_DIR__="):
            stage_dir = line.split("=", 1)[1].strip()
        elif line.startswith("__DAYLILY_STAGE_SAMPLES__="):
            samples_path = line.split("=", 1)[1].strip()
        elif line.startswith("__DAYLILY_STAGE_UNITS__="):
            units_path = line.split("=", 1)[1].strip()
        elif line.startswith("__DAYLILY_ERROR__="):
            raise CommandError(f"Remote lookup failed: {line.split('=', 1)[1]}")
    if not (stage_dir and samples_path and units_path):
        raise CommandError("Unable to determine staged config paths on the head node.")
    return RemoteConfig(stage_dir, samples_path, units_path)


def discover_stage_config(
    pem: str,
    host: str,
    stage_dir: Optional[str],
    stage_base: str,
) -> RemoteConfig:
    if stage_dir:
        target_dir = normalize_remote_path(stage_dir.rstrip("/"))
        script = f"""
set -euo pipefail
STAGE_DIR={shlex.quote(target_dir)}
if [[ ! -d "$STAGE_DIR" ]]; then
  echo "__DAYLILY_ERROR__=missing_stage_dir"
  exit 2
fi
samples_file=$(ls -1 "$STAGE_DIR"/*_samples.tsv 2>/dev/null | head -n 1)
units_file=$(ls -1 "$STAGE_DIR"/*_units.tsv 2>/dev/null | head -n 1)
if [[ -z "$samples_file" || -z "$units_file" ]]; then
  echo "__DAYLILY_ERROR__=missing_config"
  exit 3
fi
echo "__DAYLILY_STAGE_DIR__=$STAGE_DIR"
echo "__DAYLILY_STAGE_SAMPLES__=$samples_file"
echo "__DAYLILY_STAGE_UNITS__=$units_file"
"""
    else:
        stage_base_norm = normalize_remote_path(stage_base.rstrip("/"))
        script = f"""
set -euo pipefail
STAGE_BASE={shlex.quote(stage_base_norm)}
if [[ ! -d "$STAGE_BASE" ]]; then
  echo "__DAYLILY_ERROR__=missing_stage_base"
  exit 2
fi
latest_dir=$(ls -1dt "$STAGE_BASE"/*/ 2>/dev/null | head -n 1)
if [[ -z "$latest_dir" ]]; then
  echo "__DAYLILY_ERROR__=no_stage_runs"
  exit 3
fi
samples_file=$(ls -1 "$latest_dir"/*_samples.tsv 2>/dev/null | head -n 1)
units_file=$(ls -1 "$latest_dir"/*_units.tsv 2>/dev/null | head -n 1)
if [[ -z "$samples_file" || -z "$units_file" ]]; then
  echo "__DAYLILY_ERROR__=missing_config"
  exit 4
fi
echo "__DAYLILY_STAGE_DIR__=$latest_dir"
echo "__DAYLILY_STAGE_SAMPLES__=$samples_file"
echo "__DAYLILY_STAGE_UNITS__=$units_file"
"""
    result = run_remote_script(pem, host, script)
    if result.stdout:
        print(result.stdout, end="")
    if result.stderr:
        print(result.stderr, file=sys.stderr, end="")
    return parse_remote_config(result.stdout)


def format_list(values: List[str]) -> str:
    quoted = ",".join(f"'{value.strip()}'" for value in values if value.strip())
    return f"[{quoted}]"


def build_default_command(
    target: str,
    genome: str,
    jobs: int,
    aligners: List[str],
    dedupers: List[str],
    snv_callers: List[str],
    containerized: bool,
    dry_run: bool,
    extra: Optional[str],
) -> str:
    config_args = [
        f"genome_build={genome}",
        f"aligners={format_list(aligners)}",
        f"dedupers={format_list(dedupers)}",
        f"snv_callers={format_list(snv_callers)}",
    ]
    command = [
        "DAY_CONTAINERIZED=true" if containerized else "DAY_CONTAINERIZED=false",
        "dy-r",
        target,
        "-p",
        "-k",
        f"-j {jobs}",
        "--config",
        " ".join(config_args),
    ]
    if dry_run:
        command.append("-n")
    if extra:
        command.append(extra)
    return " ".join(command)


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Clone daylily-omics-analysis and launch a workflow inside tmux.",
    )
    parser.add_argument(
        "--profile",
        default=os.environ.get("AWS_PROFILE"),
        help="AWS CLI profile to use (default: $AWS_PROFILE)",
    )
    parser.add_argument("--region", help="AWS region for the cluster")
    parser.add_argument("--cluster", help="ParallelCluster name")
    parser.add_argument("--pem", help="Path to the SSH PEM key")
    parser.add_argument(
        "--stage-dir",
        help="Specific staging directory containing *_samples.tsv and *_units.tsv",
    )
    parser.add_argument(
        "--stage-base",
        default="/fsx/staged_sample_data",
        help="Base staging directory to scan when --stage-dir is omitted",
    )
    parser.add_argument(
        "--session-name",
        default="daylily-omics-analysis",
        help="Name of the tmux session to create on the head node",
    )
    parser.add_argument(
        "--destination",
        default="dayoa",
        help="Workspace destination passed to day-clone",
    )
    parser.add_argument(
        "--repository",
        default="daylily-omics-analysis",
        help="Repository key to pass to day-clone",
    )
    parser.add_argument(
        "--transport",
        choices=["https", "ssh"],
        default="https",
        help="Git transport for day-clone",
    )
    parser.add_argument(
        "--project",
        help="Project/budget to supply to dyoainit",
    )
    parser.add_argument(
        "--skip-project-check",
        action="store_true",
        help="Pass --skip-project-check to dyoainit",
    )
    parser.add_argument(
        "--genome",
        default="hg38",
        help="Genome build to activate (default: %(default)s)",
    )
    parser.add_argument(
        "--jobs",
        type=int,
        default=6,
        help="Value for dy-r -j (default: %(default)s)",
    )
    parser.add_argument(
        "--aligners",
        default="bwa2a",
        help="Comma separated list of aligners",
    )
    parser.add_argument(
        "--dedupers",
        default="dppl",
        help="Comma separated list of dedupers",
    )
    parser.add_argument(
        "--snv-callers",
        default="deep",
        help="Comma separated list of SNV callers",
    )
    parser.add_argument(
        "--target",
        default="produce_snv_concordances",
        help="Workflow target to run via dy-r",
    )
    parser.add_argument(
        "--dy-command",
        help="Override the dy-r command entirely",
    )
    parser.add_argument(
        "--snakemake-extra",
        help="Additional arguments appended to the dy-r command",
    )
    parser.add_argument(
        "--no-containerized",
        action="store_true",
        help="Disable DAY_CONTAINERIZED (enabled by default)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Add -n to the dy-r command",
    )
    return parser


def main(argv: Optional[List[str]] = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    aws_profile = args.profile
    if not aws_profile:
        raise CommandError("AWS profile is required. Set AWS_PROFILE or use --profile.")

    region = args.region or resolve_region(aws_profile)
    cluster_name = args.cluster or resolve_cluster(aws_profile, region)
    pem_file = resolve_pem_file(args.pem)
    headnode_ip = fetch_headnode_ip(aws_profile, region, cluster_name)

    stage_config = discover_stage_config(pem_file, headnode_ip, args.stage_dir, args.stage_base)

    if args.dy_command:
        dy_command = args.dy_command
    else:
        aligners = args.aligners.split(",")
        dedupers = args.dedupers.split(",")
        snv_callers = args.snv_callers.split(",")
        dy_command = build_default_command(
            target=args.target,
            genome=args.genome,
            jobs=args.jobs,
            aligners=aligners,
            dedupers=dedupers,
            snv_callers=snv_callers,
            containerized=not args.no_containerized,
            dry_run=args.dry_run,
            extra=args.snakemake_extra,
        )

    project_arg = shlex.quote(args.project) if args.project else ""
    skip_check = "true" if args.skip_project_check else "false"

    pipeline_script = f"""
set -euo pipefail
SESSION_NAME={shlex.quote(args.session_name)}
STAGE_DIR={shlex.quote(stage_config.stage_dir)}
STAGE_SAMPLES={shlex.quote(stage_config.samples_path)}
STAGE_UNITS={shlex.quote(stage_config.units_path)}
DESTINATION={shlex.quote(args.destination)}
REPO_KEY={shlex.quote(args.repository)}
TRANSPORT={shlex.quote(args.transport)}
PROJECT_VALUE={project_arg if project_arg else ""}
SKIP_PROJECT_CHECK={skip_check}
DY_COMMAND={dy_command}

analysis_root=$(python3 - <<'PYCONFIG'
from pathlib import Path
analysis_root = '/fsx/analysis_results'
config_path = Path.home() / '.config/daylily/daylily_cli_global.yaml'
if config_path.exists():
    for line in config_path.read_text().splitlines():
        line = line.strip()
        if line.startswith('analysis_root:'):
            analysis_root = line.split(':', 1)[1].strip()
            break
print(analysis_root.rstrip('/'))
PYCONFIG
)
repo_relative=$(python3 - <<'PYREPOS'
from pathlib import Path
repo_key = {shlex.quote(args.repository)}
relative = 'daylily-omics-analysis'
config_path = Path.home() / '.config/daylily/daylily_available_repositories.yaml'
if config_path.exists():
    current_key = None
    for raw in config_path.read_text().splitlines():
        stripped = raw.strip()
        if not stripped or stripped.startswith('#'):
            continue
        if stripped.endswith(':'):
            current_key = stripped[:-1].strip()
            continue
        if current_key == repo_key and stripped.startswith('relative_path:'):
            relative = stripped.split(':', 1)[1].strip()
            break
print(relative.strip())
PYREPOS
)
if [[ -z "$analysis_root" ]]; then
  echo "[ERROR] Unable to determine analysis_root." >&2
  exit 5
fi
analysis_root=${analysis_root%/}
user_dir=$(whoami)
clone_root="$analysis_root/$user_dir/$DESTINATION"
repo_path="$clone_root/$repo_relative"
mkdir -p "$clone_root"
if [[ ! -d "$repo_path/.git" ]]; then
  echo "[INFO] Cloning $REPO_KEY into $clone_root via day-clone..."
  day-clone --destination "$DESTINATION" --repository "$REPO_KEY" --which-one "$TRANSPORT"
else
  echo "[INFO] Repository already exists at $repo_path; skipping clone."
fi
if [[ ! -d "$repo_path" ]]; then
  echo "[ERROR] day-clone did not create $REPO_KEY at $repo_path" >&2
  exit 6
fi
cd "$repo_path"
mkdir -p config
cp "$STAGE_SAMPLES" config/samples.tsv
cp "$STAGE_UNITS" config/units.tsv

if [[ ! -f dyoainit ]]; then
  echo "[ERROR] dyoainit not found within $repo_path" >&2
  exit 7
fi

declare -a dyoa_args=()
if [[ -n "$PROJECT_VALUE" ]]; then
  dyoa_args+=(--project {project_arg} )
fi
if [[ "$SKIP_PROJECT_CHECK" == "true" ]]; then
  dyoa_args+=(--skip-project-check)
fi
. dyoainit "${{dyoa_args[@]}}"
dy-a slurm {shlex.quote(args.genome)} remote

echo "[INFO] Launching workflow: $DY_COMMAND"
set +e
eval "$DY_COMMAND"
workflow_status=$?
set -e
echo "[INFO] Workflow exited with status $workflow_status"
if [[ $workflow_status -ne 0 ]]; then
  echo "[ERROR] Workflow failed with status $workflow_status" >&2
fi
echo "[INFO] Attach with: tmux attach -t $SESSION_NAME"
exec bash
"""

    tmux_script = f"""
set -euo pipefail
SESSION_NAME={shlex.quote(args.session_name)}
if tmux has-session -t "$SESSION_NAME" 2>/dev/null; then
  echo "__DAYLILY_ERROR__=session_exists"
  exit 8
fi
work_script=$(mktemp)
cat <<'PAYLOAD' > "$work_script"
{pipeline_script}
PAYLOAD
tmux new-session -d -s "$SESSION_NAME" "bash -lc 'source \"$work_script\"'"
echo "__DAYLILY_SESSION__=$SESSION_NAME"
echo "__DAYLILY_WORK_SCRIPT__=$work_script"
"""

    result = run_remote_script(pem_file, headnode_ip, tmux_script)
    if result.stdout:
        print(result.stdout, end="")
    if result.stderr:
        print(result.stderr, file=sys.stderr, end="")

    for line in result.stdout.splitlines():
        if line.startswith("__DAYLILY_ERROR__="):
            raise CommandError(line.split("=", 1)[1])
        if line.startswith("__DAYLILY_SESSION__="):
            session_name = line.split("=", 1)[1].strip()
            print(f"Tmux session '{session_name}' created on the head node.")
            print(
                "Attach with: ssh -i {pem} ubuntu@{host} 'tmux attach -t {session}'".format(
                    pem=pem_file,
                    host=headnode_ip,
                    session=session_name,
                )
            )
            break
    else:
        raise CommandError("Tmux session creation did not report success.")

    return 0


if __name__ == "__main__":  # pragma: no cover
    try:
        raise SystemExit(main())
    except CommandError as exc:
        print(f"Error: {exc}", file=sys.stderr)
        raise SystemExit(1)
