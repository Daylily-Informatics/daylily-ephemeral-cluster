# Example configuration for the daylily workset monitor tool.
# Copy and edit this file before running bin/daylily-monitor-worksets.
# Use --wrap-char when generating --report term output to control column wrapping (default 100).
# Use --force-metrics when reporting to refresh cached metrics for every workset regardless of state.

aws:
  profile: default
  region: us-east-1
  # Optional session duration for profile authentication in seconds.
  session_duration_seconds: 3600

monitor:
  bucket: my-daylily-bucket
  prefix: monitoring/worksets/
  poll_interval_seconds: 60
  ready_lock_backoff_seconds: 30
  # Set to true to keep looping forever.  When false the monitor performs one scan and exits.
  continuous: true
  # Where to mirror sentinel state listings (in the same bucket/prefix).
  sentinel_index_prefix: monitoring/

cluster:
  # Reuse an existing cluster when available, otherwise create from this template.
  template_path: config/cluster-configs/example.yaml
  # Contact email exported for cluster creation when not present in template overrides.
  contact_email: you@email.com
  preferred_availability_zone: us-east-1a
  auto_teardown: false
  idle_teardown_minutes: 20
  # Optional pre-existing cluster name.  When empty a new cluster is created as required.
  reuse_cluster_name: ""
  # Optional repo tag passed to the cluster creation helper.
  repo_tag: main

pipeline:
  # Directory on the headnode where worksets will be staged.
  workdir: /fsx/data/worksets
  # Local directory where stage_samples manifests are mirrored prior to staging commands.
  local_stage_root: .
  # Optional explicit S3 bucket URI passed to staging commands.  Defaults to the monitor bucket when omitted.
  reference_bucket: s3://my-daylily-bucket/
  # SSH identity file used when running commands on the head node.
  ssh_identity_file: ~/.ssh/daylily-headnode.pem
  # SSH user name for the head node connection.
  ssh_user: ubuntu
  # Additional SSH/SCP options when connecting to the head node.
  ssh_extra_args: []
  # Command that prepares staging files on the headnode.  {analysis_samples} is replaced at runtime.
  stage_command: ./bin/daylily-stage-samples-from-local-to-headnode --profile {profile} --region {region} --reference-bucket {reference_bucket} {analysis_samples}
  # Command that creates the pipeline working tree before execution.  The day-clone output
  # is parsed to determine the working directory for subsequent commands.
  clone_command: echo cloning && day-clone {clone_args}
  # Command prefix that initialises the analysis environment.  The dy-r suffix from the workset yaml
  # is appended to the end of this string.
  run_prefix: "source dyoainit && source bin/day_activate slurm hg38  && DAY_CONTAINERIZED=0 ./bin/day_run "
  # Maximum time (in minutes) to wait for the pipeline success sentinel before aborting.
  pipeline_timeout_minutes: 240
  # Command that exports results back to S3 after completion.
  export_command: ./bin/daylily-export-fsx-to-s3-from-local --cluster {cluster} --target-uri {target_uri} --region {region} --profile {profile} --output-dir {output_dir}
  # Command executed before entering the pipeline directory on the head node.
  login_shell_init: source ~/.bashrc && echo "Ready to run"
  # Prefix used when generating tmux session names for launched pipelines.
  tmux_session_prefix: daylily
  # Shell invoked at the end of the tmux command to keep the session alive for inspection.
  tmux_keepalive_shell: bash


# optional if you want the monitor to fetch your repo/branch on the headnode
day-clone: "--repo Daylily-Informatics/daylily-omics-analysis --branch main"

# required: appended to run_prefix
dy-r: "--name runme14 --samples config/samples.tsv --outdir results/runme14 --profile slurm --jobs 192 --rerun-incomplete"

# optional export
export_uri: "s3://my-daylily-bucket/monitoring/worksets/runme14/results/"

