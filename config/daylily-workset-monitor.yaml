# Example configuration for the daylily workset monitor tool.
# Copy and edit this file before running bin/daylily-monitor-worksets.

aws:
  profile: default
  region: us-east-1
  # Optional session duration for profile authentication in seconds.
  session_duration_seconds: 3600

monitor:
  bucket: my-daylily-bucket
  prefix: monitoring/worksets/
  poll_interval_seconds: 60
  ready_lock_backoff_seconds: 30
  # Set to true to keep looping forever.  When false the monitor performs one scan and exits.
  continuous: true
  # Where to mirror sentinel state listings (in the same bucket/prefix).
  sentinel_index_prefix: monitoring/

cluster:
  # Reuse an existing cluster when available, otherwise create from this template.
  template_path: config/cluster-configs/example.yaml
  preferred_availability_zone: us-east-1a
  auto_teardown: false
  idle_teardown_minutes: 20
  # Optional pre-existing cluster name.  When empty a new cluster is created as required.
  reuse_cluster_name: ""

pipeline:
  # Directory on the headnode where worksets will be staged.
  workdir: /fsx/data/worksets
  # Command that prepares staging files on the headnode.  {analysis_samples} is replaced at runtime.
  stage_command: ./bin/daylily-stage-samples-from-local-to-headnode --profile {profile} --region {region} --pem ~/.ssh/daylily-headnode.pem --cluster {cluster} {analysis_samples}
  # Command that creates the pipeline working tree before execution.
  clone_command: day-clone {clone_args}
  # Command prefix that initialises the analysis environment.  The dy-r suffix is appended from the workset yaml.
  run_prefix: ". dyoinit && dy-a slurm hg38 && dy-r "
  # Command that exports results back to S3 after completion.
  export_command: ./bin/daylily-export-fsx-to-s3 --cluster {cluster} --target-uri {target_uri}
